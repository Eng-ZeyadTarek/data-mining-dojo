{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answring some questions about data mining subject\n",
    "\n",
    "##### 1- What is another preferred name of data mining?\n",
    "- Another preferred name is Knowledge discovery from databases (KDD).\n",
    " \n",
    "                                                    -----------------------------\n",
    "\n",
    "\n",
    "##### 2- Why Data Mining is a misnomer? \n",
    "- It's considered minsomer because we are not search for data itself because we already have it but it's dedicated to extract a knowldege and patterens from a huge amount of data.\n",
    " \n",
    "                                                    -----------------------------\n",
    "                                                    \n",
    "##### 3- What is the general knowledge discovery process?\n",
    "- it's the process that if we follow its steps we will discover meaningful knowledge and patterns from huge data, and emphasizes the \"high-level\" application of particular data mining methods.\n",
    "\n",
    "And its steps are:\n",
    "1. Determine the domain.\n",
    "2. Understanding the problem.\n",
    "3. Get the task-relevant data from the data warehouse or databases.\n",
    "4. Understanding the data.\n",
    "5. Cleaning the data (remove noise, null values, duplicates, convert their features to the proper data type).\n",
    "6. Choosing the data mining goal (classification, regression, clustring) and select the best model to reach this goal. (choose best hyperparameters, etc...)\n",
    "7. Data mining\n",
    "8. Pattern evaluation\n",
    "\n",
    "                                                    -----------------------------\n",
    "\n",
    "##### 4- What is the difference between a data engineer and data scientist/AI engineer?\n",
    "- They have common things when it comes to skills and mindset but the main difference between data engineer and data scientist is that the data engineer is someone who is responsible for handling and storing data into databases and large-scale processing systems also develops, constructs, tests and maintains architectures and delivering task-relevant data from source to its required destination, on the other hand, the data scientist is responsible for clean and organize and interpret the non-meaningful data using some mathematical and statistical methods also extract knowledge and gain insights from it using some business solutions models. wherease AI engineers perform include designing neural networks, performing computational analysis, creating mathematical models and implementing algorithms that help machines learn how to do specific tasks and also depoly the final model into real bussiness application or API.\n",
    "\n",
    "                                                    -----------------------------\n",
    "\n",
    "##### 5- In data mining, what is the difference between prediction and categorization?\n",
    "- the main difference between prediction and categorization is that categorization (classification) is to determine the (categorical) labels (discrete value) of objects whose class label is unknown or classify these observations into their right categories like the type of fruit, on the other hand, prediction (regression) about predicts the unknown numerical value (continuous value) like predicting the price of a new house and it's mostly used to predict the value of something in the future.\n",
    "\n",
    "                                                    -----------------------------\n",
    "\n",
    "\n",
    "##### 6- What is CIA principle and how can we use it to access the security/privacy aspect of the AI system/pipelines?\n",
    "- The CIA triad is a concept for guiding information security policy within an organization and it named like that because it consists of three important key terms confidentiality (is a set of rules that restricts information access), integrity (make sure that the information in the organization is accurate and reliable), availability(is a guarantee that only authorized people will have dependable access to information). and we can use it in our AI pipelines to ensure that AI pipeline follow CIA guidlines or to check what is the type of attaks that we are under and fix the situation quickly.\n",
    "\n",
    "types of attacks:\n",
    "\n",
    "- Adversarial (front-end) - Confidentiality\n",
    "- Backdoor/data poisoning (data source) – Integrity\n",
    "- Training data reconstruction – Confidentiality (privacy)\n",
    "- Steeling the model - Confidentiality\n",
    "- Membership inference - Confidentiality (privacy)\n",
    "\n",
    "                                                    -----------------------------\n",
    "\n",
    "##### 7- Why data science/machine learning is a bad idea in the context of information security?\n",
    "\n",
    "because data science is a data-driven approach so it depends on the data to work and that may be increased risk of data breach and fine and by that you may build a bad model, or may be fed a wrong informaiton to your model, also you have to make sure that you're folllow data privacy guidlines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation:\n",
    "\n",
    "here we have a dataset from Kaggle which was originally scraped from Wish E-Commerce Platform. It contains some input features like (price, retail_price, product_size, product_colour) and the output here is rating. so our job here to build the model to make us able to answering some quesitons like.\n",
    "\n",
    "- What are the top selling products?\n",
    "- Which are the most important features that help us predicting whether the product will succuess or nor?sold?\n",
    "- what's the expected rating of product before listing it out into the site?\n",
    "\n",
    "The first challenge here is that the data is not clean and we need a lot of preprocessing on it.\n",
    "\n",
    "The second challenge here is that the data is imblanced data set (we have a lot 4 rating and only 11 rating of 2).\n",
    "\n",
    "The impact of solving this problem that is you can make an educated guess about how likely people are to like your product without actually putting it on the market. In addition, by doing so, we may better determine under what circumstances a product will be highly rated, as well as the wish.com consumer base.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "### Data mining function\n",
    "\n",
    "- **classification & prediction**\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental protocol:\n",
    "\n",
    "first: Data Processing:\n",
    "\n",
    "It’s a key step in Machine Learning project to ensure that data is transformed ,clean, and easy to use for analytical purpose.\n",
    "Below are some important/key steps:\n",
    "1. Drop irrelevant and unnecessary features.\n",
    "2. Check if there is any null values and replace them.\n",
    "3. Create new features from existing features if needed\n",
    "4. Clean categorical variables.\n",
    "\n",
    "second: convert categorical and string columns to numerical columns.\n",
    "\n",
    "third: start buliding your models and choose the best one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: let's import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x2589a9226d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#to match some patternes \n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "np.printoptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second: read wish dataset\n",
    "\n",
    "- step 1: read train dataset\n",
    "- step 2: read test dataset\n",
    "- step 3: drop row with rating value 6 because the rating range from 1 to 5\n",
    "- step 4: merge both datasets because there's values in test set not in train set in vice versa so when you'll some columns to one hot encoding the number features in both datasets when you spilt them again should be equal to each others.\n",
    "\n",
    "- step 5: have a look at your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: you'll find the data set in https://www.kaggle.com/competitions/cisc-873-dm-f22-a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1572 entries, 0 to 1571\n",
      "Data columns (total 24 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   price                         1572 non-null   float64\n",
      " 1   retail_price                  1572 non-null   int64  \n",
      " 2   units_sold                    1572 non-null   int64  \n",
      " 3   uses_ad_boosts                1572 non-null   int64  \n",
      " 4   rating                        1093 non-null   float64\n",
      " 5   rating_count                  1572 non-null   int64  \n",
      " 6   badges_count                  1572 non-null   int64  \n",
      " 7   badge_local_product           1572 non-null   int64  \n",
      " 8   badge_product_quality         1572 non-null   int64  \n",
      " 9   badge_fast_shipping           1572 non-null   int64  \n",
      " 10  tags                          1572 non-null   object \n",
      " 11  product_color                 1531 non-null   object \n",
      " 12  product_variation_size_id     1558 non-null   object \n",
      " 13  product_variation_inventory   1572 non-null   int64  \n",
      " 14  shipping_option_name          1572 non-null   object \n",
      " 15  shipping_option_price         1572 non-null   int64  \n",
      " 16  shipping_is_express           1572 non-null   int64  \n",
      " 17  countries_shipped_to          1572 non-null   int64  \n",
      " 18  inventory_total               1572 non-null   int64  \n",
      " 19  has_urgency_banner            473 non-null    float64\n",
      " 20  origin_country                1555 non-null   object \n",
      " 21  merchant_rating_count         1572 non-null   int64  \n",
      " 22  merchant_rating               1572 non-null   float64\n",
      " 23  merchant_has_profile_picture  1572 non-null   int64  \n",
      "dtypes: float64(4), int64(15), object(5)\n",
      "memory usage: 294.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('train_new.csv')#\n",
    "df2= pd.read_csv('test_new.csv')#\n",
    "\n",
    "df1 = df1[df1['rating'] != 6]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "frames = [df1, df2]\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "df = df.drop(['theme','id','urgency_text','merchant_title','merchant_info_subtitle','currency_buyer','merchant_name','merchant_id','merchant_profile_picture','crawl_month'],axis=1)\n",
    "\n",
    "\n",
    "#this columns we'll change their types to categorical datatype after cleaning them.\n",
    "columns = ['merchant_has_profile_picture','has_urgency_banner','badge_local_product','badge_product_quality','badge_fast_shipping','shipping_is_express','product_variation_size_id','product_color']\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### third: clean \"has_urgency_banner column\".\n",
    "\n",
    "we will replace null values in this column with 0, since it has only 1 and null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_urgency_banner'] = df['has_urgency_banner'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fourth: Normalize numeric columns values between 0 and 1 to handel outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_max_scaler constructor\n",
    "min_max_scaler = MinMaxScaler()\n",
    "#extract integer columns\n",
    "myCols = df.select_dtypes(include='int64').columns\n",
    "#normlize integer columns\n",
    "df[myCols] = min_max_scaler.fit_transform(df[myCols])\n",
    "#normalize float columns\n",
    "df[['price','merchant_rating']] = min_max_scaler.fit_transform(df[['price','merchant_rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fifth: polt each value count in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms_and_countplots(column, data):\n",
    "        if data[column].dtype not in ['int64', 'float64']:\n",
    "            f, axes = plt.subplots(1,1,figsize=(15,5))\n",
    "            sns.countplot(x=column, data = data)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.suptitle(column,fontsize=20)\n",
    "            plt.show()\n",
    "        else:\n",
    "            g = sns.FacetGrid(data, margin_titles=True, aspect=4, height=3)\n",
    "            g.map(plt.hist,column,bins=100)\n",
    "            plt.show()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sixth: create submission file.\n",
    "\n",
    "note: the file that you'll pass it to that function should contain columns names (id, rating) and id values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_file(file_path,predicted_values):\n",
    "    df = pd.read_csv(file_path) #empty file with column names\n",
    "    df['rating'] = predicted_values #prediction values\n",
    "    df.to_csv(file_path, index=False) #write changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seventh: replace low-frequancy values with value of my choice\n",
    "\n",
    "As one can see, there is a lot of distinct values in some column . Most of these values are either redundant or irrelevant due to low frequency count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_values(col,value_to_be_replaced):\n",
    "    #extract values with count 2 or less than 2\n",
    "    bool_lst = [df[col].value_counts() <= 2]\n",
    "    lst = list(df[col].value_counts().index[tuple(bool_lst)])\n",
    "    #replace them with value of my choice\n",
    "    df[col].replace(dict.fromkeys(lst, value_to_be_replaced),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eighth: clean size column.\n",
    "\n",
    " - step 1: replace null values with 'ANOTHER_SIZE' value.\n",
    " - step 2: replace some redundant values with true values based on regular expression pattern ex ('SIZEL' will be 'L', SIZE\\S will 'S').\n",
    " - step 3: replace some values like XXXXS will be 4XS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_size(cell):\n",
    "    \n",
    "    #replace null values with \"ANOTHER_SIZE\"\n",
    "    if not isinstance(cell,str) and math.isnan(cell):\n",
    "        return \"ANOTHER_SIZE\"\n",
    "\n",
    "    #set some regex patterns    \n",
    "    pattern = re.compile(r'(\\bL\\b|(?<=[^MF])L|\\bX+S|S\\b|\\bS\\b|\\dXS|\\bM\\b|\\d?X+L)')\n",
    "    match = re.search(pattern,cell)\n",
    "    #if value doesn't match the pattern don't do anything.\n",
    "    if match is not None:\n",
    "        cell = match[0]\n",
    "\n",
    "    #if value has more than X replace the value the number of X's       \n",
    "    if cell.count('X') > 1 and (cell.endswith('XS') or cell.endswith('XL')):\n",
    "        cell = f\"{cell.count('X')}X{cell[-1]}\"\n",
    "        \n",
    "    return cell    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S                641\n",
      "XS               356\n",
      "M                200\n",
      "XXS              100\n",
      "L                 49\n",
      "                ... \n",
      "XXXXXL             1\n",
      "20PCS-10PAIRS      1\n",
      "Size-5XL           1\n",
      "Size/S             1\n",
      "36                 1\n",
      "Name: product_variation_size_id, Length: 106, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S               703\n",
       "XS              369\n",
       "M               206\n",
       "2XS             107\n",
       "ANOTHER_SIZE     59\n",
       "L                58\n",
       "2XL              19\n",
       "XL               18\n",
       "4XL              10\n",
       "3XS               6\n",
       "3XL               4\n",
       "5XL               4\n",
       "10 ML             3\n",
       "34                3\n",
       "33                3\n",
       "Name: product_variation_size_id, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of product_variation_size_id before the cleaning.\n",
    "print(df['product_variation_size_id'].value_counts())\n",
    "df['product_variation_size_id'] = df['product_variation_size_id'].str.upper()\n",
    "df.product_variation_size_id = df.product_variation_size_id.apply(clean_product_size)\n",
    "replace_less_values('product_variation_size_id','ANOTHER_SIZE')\n",
    "#Count of product_variation_size_id after the cleaning.\n",
    "df.product_variation_size_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nineth: clean product color column\n",
    "\n",
    "- step 1: convert all values to upper case.\n",
    "- step 2: replace all null values to \"ANOHTER COLOR\"\n",
    "- step 3: replace some low-frequancy values to thier true value manually EX (GRAY (low-frequany count) will be GREY (high-frequancy count), ROSE RED will be RED).\n",
    "- step 4: replace low-frequancy values with \"ANOTHER_COLOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLACK               305\n",
       "WHITE               258\n",
       "ANOTHER_COLOR       137\n",
       "YELLOW              106\n",
       "BLUE                104\n",
       "PINK                102\n",
       "RED                 100\n",
       "GREEN                90\n",
       "GREY                 82\n",
       "PURPLE               54\n",
       "ARMY GREEN           34\n",
       "NAVYBLUE             28\n",
       "WINERED              28\n",
       "ORANGE               27\n",
       "BROWN                26\n",
       "BEIGE                14\n",
       "LIGHTBLUE            12\n",
       "WHITE & GREEN        10\n",
       "SKYBLUE               8\n",
       "ROSERED               8\n",
       "DARKBLUE              6\n",
       "FLORAL                5\n",
       "LIGHTPINK             4\n",
       "BLACK & GREEN         4\n",
       "FLUORESCENTGREEN      4\n",
       "LEOPARD               4\n",
       "CAMOUFLAGE            3\n",
       "BLACK & WHITE         3\n",
       "LIGHTGREEN            3\n",
       "ORANGE-RED            3\n",
       "Name: product_color, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product_color'] = df['product_color'].str.upper()\n",
    "df['product_color'] = df['product_color'].fillna('ANOTHER_COLOR')\n",
    "df[\"product_color\"] = df[\"product_color\"].replace({\n",
    "    \"GRAY\": \"GREY\",\n",
    "    'ARMYGREEN': 'ARMY GREEN',\n",
    "    'LIGHTGRAY': 'LIGHTGREY', \n",
    "    'MULTICOLOR':'ANOTHER_COLOR',\n",
    "    'CLARET':'RED',\n",
    "    'ROSEGOLD':'PINK',\n",
    "    'VIOLET':'PURPLE',\n",
    "    'IVORY':'WHITE',\n",
    "    'COFFEE':'BROWN',\n",
    "    'KHAKI': 'BROWN',\n",
    "    'NAVY':'BLUE',\n",
    "    'LIGHTKHAKI':'YELLOW',\n",
    "    'ROSE':'RED',\n",
    "    'ROSE RED':'RED'\n",
    "})\n",
    "\n",
    "replace_less_values('product_color','ANOTHER_COLOR')\n",
    "\n",
    "df['product_color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tenth: clean origin_country column\n",
    "\n",
    "- step 1: replace 'null' values with 'OTHER'\n",
    "- step 2: replace low-frequancy count values with 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CN       1515\n",
       "US         31\n",
       "OTHER      21\n",
       "VE          5\n",
       "Name: origin_country, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['origin_country'] = df['origin_country'].fillna('OTHER')\n",
    "replace_less_values('origin_country','OTHER')\n",
    "df.origin_country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the products with ad boosts features has high rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='uses_ad_boosts', ylabel='count'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df, x='uses_ad_boosts', hue='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the plot it's obvious that the most products that has ad boosts features has high rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1572 entries, 0 to 1571\n",
      "Data columns (total 24 columns):\n",
      " #   Column                        Non-Null Count  Dtype   \n",
      "---  ------                        --------------  -----   \n",
      " 0   price                         1572 non-null   float64 \n",
      " 1   retail_price                  1572 non-null   float64 \n",
      " 2   units_sold                    1572 non-null   float64 \n",
      " 3   uses_ad_boosts                1572 non-null   float64 \n",
      " 4   rating                        1093 non-null   float64 \n",
      " 5   rating_count                  1572 non-null   float64 \n",
      " 6   badges_count                  1572 non-null   float64 \n",
      " 7   badge_local_product           1572 non-null   category\n",
      " 8   badge_product_quality         1572 non-null   category\n",
      " 9   badge_fast_shipping           1572 non-null   category\n",
      " 10  tags                          1572 non-null   object  \n",
      " 11  product_color                 1572 non-null   category\n",
      " 12  product_variation_size_id     1572 non-null   category\n",
      " 13  product_variation_inventory   1572 non-null   float64 \n",
      " 14  shipping_option_name          1572 non-null   object  \n",
      " 15  shipping_option_price         1572 non-null   float64 \n",
      " 16  shipping_is_express           1572 non-null   category\n",
      " 17  countries_shipped_to          1572 non-null   float64 \n",
      " 18  inventory_total               1572 non-null   float64 \n",
      " 19  has_urgency_banner            1572 non-null   category\n",
      " 20  origin_country                1572 non-null   object  \n",
      " 21  merchant_rating_count         1572 non-null   float64 \n",
      " 22  merchant_rating               1572 non-null   float64 \n",
      " 23  merchant_has_profile_picture  1572 non-null   category\n",
      "dtypes: category(8), float64(13), object(3)\n",
      "memory usage: 211.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df[columns] = df[columns].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of that now our data is clean, now let's convert categorical columns and string columns to numeric columns to make the models able to learn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First: tags columns\n",
    "\n",
    "tags column is an string column and has a lot of unique values so it is not a good idea to convert it to numerical column using one hot encoding method, instead we will use CountVectorizer method.\n",
    "\n",
    "CountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1572, 2067)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#build object\n",
    "vect = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "#define the column\n",
    "text = df['tags']\n",
    "\n",
    "#extract words\n",
    "vect.fit(text)\n",
    "\n",
    "#Transform tags to vectors\n",
    "train = vect.fit_transform(text)\n",
    "\n",
    "train.toarray()\n",
    "\n",
    "#convert train array to dataframe\n",
    "data = pd.DataFrame(train.toarray(), columns=vect.get_feature_names())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge new columns to the dataframe and drop tags column\n",
    "\n",
    "df = pd.concat([df.drop(['tags'],axis=1),data], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second: convert categorical columns to numerical columns using one hot encoding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical columns\n",
    "features = [\"product_variation_size_id\",\"shipping_option_name\",'product_color','origin_country']\n",
    "\n",
    "df = pd.get_dummies(df, columns = features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split X_test\n",
    "X_test = df.iloc[df1.shape[0]:,:]\n",
    "#X_train split\n",
    "\n",
    "X_train = df.iloc[:df1.shape[0],:]\n",
    "#drop rating column from X_test (anyway all values in X_test rating is null because orginal test set doesn't have rating column)\n",
    "X_test = X_test.drop(['rating'],axis=1)\n",
    "\n",
    "#drop any duplicated values from X_train\n",
    "X_train = X_train.drop_duplicates()\n",
    "#extract labels\n",
    "y_train = X_train['rating']\n",
    "#drop rating column from X_train.\n",
    "X_train = X_train.drop(['rating'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model: Decision tree using grid search to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "best hyperparameters found are DecisionTreeClassifier(max_depth=3, min_samples_leaf=10)\n",
      "Counter({4.0: 443, 5.0: 26, 3.0: 10})\n"
     ]
    }
   ],
   "source": [
    "DT_param_1 = {\n",
    "    'max_depth': [2, 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'min_samples_split':[2,3,4,5],\n",
    "    'criterion': [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "model_dt_1 = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model_dt_1, param_grid= DT_param_1, cv=5, n_jobs=-1, verbose=3)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "predicted_test_dt = grid_search.predict(X_test)\n",
    "print(f\"best hyperparameters found are {grid_search.best_estimator_}\")\n",
    "\n",
    "predicted_test_dt.astype(float)\n",
    "print(Counter(predicted_test_dt))\n",
    "create_sample_file('sample Descision tree with hyperparmeter 1.csv',predicted_test_dt)\n",
    "\n",
    "check_dt_1 = pd.read_csv('sample SVM with hyperparameter 1.csv')\n",
    "print(Counter(check_dt_1['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model: Decision tree with tuning hyperparameters manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4.0: 443, 5.0: 26, 3.0: 10})\n"
     ]
    }
   ],
   "source": [
    "#model_dt_2 = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 2, criterion = 'gini') first trial: f1-score 76.643, I will decrease the max-depth values and increase the min_samples_leaf by one\n",
    "#model_dt_2 = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 3, criterion = 'gini') second trial: f1-score 77.643, I will change the criterion from gini to entropy\n",
    "#model_dt_2 = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 3, criterion = 'entropy') second trial: f1-score 79.03, I will change the min_sample_leaf to 2\n",
    "model_dt_2 = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 3, min_samples_split = 6, criterion = 'entropy') #best hyper parameters with f1 score 79.991\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_dt_2.fit(X_train, y_train)\n",
    "predicted_test_dt_2 = grid_search.predict(X_test)\n",
    "\n",
    "predicted_test_dt_2.astype(float)\n",
    "print(Counter(predicted_test_dt_2))\n",
    "create_sample_file('sample Descision tree with hyperparmeter 2.csv',predicted_test_dt_2)\n",
    "\n",
    "#check_dt_2 = pd.read_csv('sample SVM with hyperparameter 1.csv')\n",
    "#print(Counter(check_dt_2['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model: SVM with using grid search to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "best hyperparameters found are SVC(gamma=0.1, probability=True)\n",
      "Counter({4.0: 437, 3.0: 26, 5.0: 15, 2.0: 1})\n",
      "Counter({4.0: 437, 3.0: 26, 5.0: 15, 2.0: 1})\n"
     ]
    }
   ],
   "source": [
    "svm_param_1 = {\n",
    "    'C': [0.01,0.1,1.0],\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma': [0.5,0.1,1]\n",
    "}\n",
    "\n",
    "model_svm_1 = SVC(probability=True)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model_svm_1, param_grid= svm_param_1, cv=5, n_jobs=-1, verbose=3)\n",
    "\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "predicted_test_svm = grid_search.predict(X_test)\n",
    "print(f\"best hyperparameters found are {grid_search.best_estimator_}\")\n",
    "\n",
    "predicted_test_svm.astype(float)\n",
    "print(Counter(predicted_test_svm))\n",
    "create_sample_file('sample SVM with hyperparameter 1.csv',predicted_test_svm)\n",
    "\n",
    "check_svm_1 = pd.read_csv('sample SVM with hyperparameter 1.csv')\n",
    "print(Counter(check_svm_1['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth model: SVM with tuning hyperparameters manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4.0: 437, 3.0: 26, 5.0: 15, 2.0: 1})\n",
      "Counter({4.0: 437, 3.0: 26, 5.0: 15, 2.0: 1})\n"
     ]
    }
   ],
   "source": [
    "#model_svm_2 = SVC(C= 0.01, kernel = 'linear', gamma= 0.5,probability=True) trial 1: with f1 score= 0.65321 you need to make the model more complex (change the kernel)\n",
    "#model_svm_2 = SVC(C= 0.1, kernel='poly',degree= 4 , probability=True) trial 2: with f1 score = 0.683213 you need to change the kernel to rbf\n",
    "#model_svm_2 = SVC(C = 10, kernel ='rbf',probability=True) trial 3: with f1 score 0.653213 change the kernel to poly and make it more simpler.\n",
    "model_svm_2 = SVC(C = 1, kernel ='poly',degree = 2,probability=True) #best f1 score with 0.77500\n",
    "\n",
    "\n",
    "model_svm_2.fit(X_train, y_train)\n",
    "predicted_test_svm_2 = grid_search.predict(X_test)\n",
    "\n",
    "predicted_test_svm_2.astype(float)\n",
    "print(Counter(predicted_test_svm_2))\n",
    "create_sample_file('sample SVM with hyperparameter 2.csv',predicted_test_svm_2)\n",
    "\n",
    "check_svm_2 = pd.read_csv('sample SVM with hyperparameter 2.csv')\n",
    "print(Counter(check_svm_2['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth model: Multinomial with tuning hyperparameters manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4.0: 399, 5.0: 49, 3.0: 28, 2.0: 3})\n",
      "float64\n",
      "Counter({4.0: 399, 5.0: 49, 3.0: 28, 2.0: 3})\n"
     ]
    }
   ],
   "source": [
    "#clf = MultinomialNB(alpha=0.01) trial 1: f1-score 60.1231 a lot of missclassified values, maybe it's an overfitting use a highr alpha\n",
    "\n",
    "#clf = MultinomialNB(alpha= 0.05) trial 2:  f1-score 70.31 you still need to use a higher alpha\n",
    "\n",
    "#clf = MultinomialNB(alpha = 0.1) trial 3: f1-score 70.31 you still need to use a higher alpha\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.5) # last trial best f1 score 0.75732\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predicted_test_NV = clf.predict(X_test)\n",
    "\n",
    "print(Counter(predicted_test_NV))\n",
    "\n",
    "\n",
    "predicted_test_NV.astype(float)\n",
    "\n",
    "print(predicted_test_NV.dtype)\n",
    "create_sample_file('Sample NV.csv',predicted_test_NV)\n",
    "check_NV = pd.read_csv('Sample NV.csv')\n",
    "print(Counter(check_NV['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal solution for this problem is to use decision tree classifier"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
