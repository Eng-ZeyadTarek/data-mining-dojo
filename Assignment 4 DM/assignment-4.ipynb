{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Assignment 4"]},{"cell_type":"markdown","metadata":{},"source":["## Data mining questions\n","\n","1- Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?\n","\n","- No, It's not good, because the model is stateless, also since the sequential data is dependent on each other, but the neural network assumes that the data is non-sequential, it isloate features from each other and start training each feature alone, also it's not good for image preprocessing and classification, because it's not good at image feature extraction, and there will be a lot of learnable parameters and that may cause overfitting.\n","\n","<hr>\n","\n","2- What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?\n","\n","- They are unstable behaviourx happen while model goes backward to learn, gradient vanishing, the gradient get smaller and smaller get smaller and smaller untill it reach to zero, on other hand, gradient explosion, the gradient get larger and larger and this will make weights updated to worset and worset values.<br><br>LSTM has three gates (forget gate, input gate, output gate [outputs a prediction vector h(x) at k-th time step]), As a result, LSTM features a distinct gradient structure that allows direct access to the activations of the forget gate, allowing the network to encourage desired behaviour from the error gradient via frequent gate updates at each time step of the learning process, and then throw cell state **(∂s<sub>t</sub>'∂s<sub>t</sub>=∏<sub>t</sub>1t'-tσ(v<sub>t</sub>+k))** ,*v<sub>t</sub>* the output of forget state but still LSTM will suffer from vanishing gradients as well, also LSTM's activation function has the derivative of 1 so it won't neither vanish or explode.<br><br>GRU also has similar sturcture of LSTM but with only 2 Gates (reset and update gate) it handel vanishing and exploding gradient a little bit better than LSTM but it may also suffer from this problem.\n","\n","<hr>\n","\n","3- What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?\n","\n","- **Multi-task learning (MTL)** is a machine learning discipline in which numerous learning problems are handled concurrently, in other hand **Multi-Modality** learning makes use of data from multiple sources to produce intelligent predictions, in this assignment we have a special dataset where we have two inputs *(summary and image of some real estates)* and *(type and categorical price of this real estates)* I leveraged from these types of learning and I built a model and trained it on these inputs at the same time, and I could predict the values of type and price of the state.\n","\n","<hr>\n","\n","4- What is the difference among xgboost, lightgbm and catboost?\n","\n","- All of these algorithms lies under gradient boosting algorithm. They differ in their implementation of the boosted trees algorithm, as well as their technological compatibility and limits, Trees grow depth-wise in XGBoost, while trees grow leaf-wise in LightGBM, for substitution,<br><br> \n"," - lightGBM supports gradient-based one-sided sampling (GOSS), which selects the split by combining all instances with big gradients (i.e., large errors) and a random sample of examples with small gradients. To maintain the same data distribution when calculating the information gain.<br><br> \n"," - Catboost offers a new technique called Minimal Variance Sampling (MVS), which is a weighted sampling version of Stochastic Gradient Boosting. Catboost provides the Minimal Variance Sample (MVS) technique, which is a weighted sampling variation of Stochastic Gradient Boosting,<br><br> \n"," - Xgboosting doesn't introduce any weighted sampling techniques..\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Problem formulation\n","\n","When hosts prepare to post a new advertisement about residence place on the Airbnb site is, how much should they ask for?, How much of this apartment?, etc, also for guests, they want to find a suitable place to stay at it affordable price..., So we want to build a neural network model to make predict the type of real estate and its price, our inputs here are summary for this real estate and its image and the outputs here are a type of this real estate and its price. *(training dataset:- *7627 samples)*, *(test dataset:- 7360 samples).\n","\n","#### Note:- **you will find the dataset at https://www.kaggle.com/competitions/cisc-873-dm-f22-a4**\n","\n","### Challengs:\n","\n","- The size of training dataset is approximately equal to test dataset and it will be smaller when we drop null and duplicated values from training dataset.\n","- Null values in summary feature in training dataset.\n","- summary feature in both training dataset and test dataset is multilingual.\n","- imbalanced labels distribution.\n","- multi-prediction problem.\n","- huge time taken to train with cpu.\n","<br><hr>\n","\n","### Impact:\n","\n","- It will help hosts to put the true and suitable price for their residence place and that will increase their sales, also it will help guests to predict the night price for the place that they are looking for it to stay in it at holidays or in the work trip days.\n","<br><hr>\n","\n","### Data mining function:\n","\n","1. load the data from the source.\n","2. preprocess and manipulating the data.\n","3. build and train the model.\n","4. classification and prediction.\n","5. get insights from the results.\n","<br><hr>\n","\n","### Ideal solution: \n","\n","For me using a bert model *(pre-trained model)* was a great choice, it got the highest score for me on kaggle, and I think that's because a lot of learnable parameters.\n","\n","Actually this model is the best model that you can use it for this kind of problems, bert model is a great semantic and that because a lot of learnable parameters, more than 108 million trainable parameters. kaggle score (68.546), and I could got better result if I tried to tune the hyperparameters of this model."]},{"cell_type":"markdown","metadata":{},"source":["## Experimental protocol\n","\n","1. load train and test data.\n","2. some visualization and description to understand the data.\n","2. preprocessing the data\n","    1. preprocessing the data to build models from scratch.\n","        1. translate text data. (summary feature).\n","        2. remove null and duplicated values from summary feature in training dataset.\n","        3. translate summary feature (because it's multilingual data).\n","        4. resize images into (64\\*64\\*3) shape (to preserve more features as we could).\n","        5. tokenize each text data in summary feature.\n","        6. convert each text into unique id sequence.\n","    2. preprocessing the data to build a pre-trained model.\n","        1. translate text data. (summary feature).\n","        2. remove null and duplicated values from summary feature in training dataset.\n","        3. translate summary feature (because it's multilingual data).\n","        4. tokenize each text data in summary feature with pre-trained tokenizer.\n","        5. convert each text into unique id sequence.\n","3. build models and plot them.\n","4. train each model.\n","5. plot each model on training loss and validation loss.\n","6. plot each model on training accuracy and validation accuracy."]},{"cell_type":"markdown","metadata":{},"source":["### REF:\n","\n","- https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n","- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n","- https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm\n","- https://huggingface.co/docs/transformers/model_doc/bert\n","- 873_mml_mtl lab\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:47.289796Z","iopub.status.busy":"2022-03-26T11:04:47.289106Z","iopub.status.idle":"2022-03-26T11:04:57.891685Z","shell.execute_reply":"2022-03-26T11:04:57.890899Z","shell.execute_reply.started":"2022-03-26T11:04:47.289672Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","import os\n","import pathlib\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","from tqdm.notebook import tqdm\n","\n","#to translate the text data\n","!pip install googletrans\n","!pip install googletrans==3.1.0a0 --user\n","from googletrans import Translator\n","import cv2\n","\n","\n","#bert tokenizer and bert model\n","from transformers import BertTokenizer\n","from transformers import TFBertModel\n","\n","#deep learning packages\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, LSTM, Dropout, Bidirectional, GRU\n","from tensorflow.keras.optimizers import Adam, Nadam\n","from keras.models import Model\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pprint import pprint\n","\n","#nlp\n","\n","from nltk.stem import WordNetLemmatizer\n","import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('wordnet')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load the data from the source"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:57.894804Z","iopub.status.busy":"2022-03-26T11:04:57.894016Z","iopub.status.idle":"2022-03-26T11:04:58.049767Z","shell.execute_reply":"2022-03-26T11:04:58.048893Z","shell.execute_reply.started":"2022-03-26T11:04:57.894767Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(\"../input/airbnb/train_xy.csv\")\n","test_df = pd.read_csv(\"../input/airbnb/test_x.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.051436Z","iopub.status.busy":"2022-03-26T11:04:58.051181Z","iopub.status.idle":"2022-03-26T11:04:58.084961Z","shell.execute_reply":"2022-03-26T11:04:58.084111Z","shell.execute_reply.started":"2022-03-26T11:04:58.051402Z"},"trusted":true},"outputs":[],"source":["#describe object features in training data\n","train_df[['summary','image','type']].describe()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.086715Z","iopub.status.busy":"2022-03-26T11:04:58.086435Z","iopub.status.idle":"2022-03-26T11:04:58.101867Z","shell.execute_reply":"2022-03-26T11:04:58.101019Z","shell.execute_reply.started":"2022-03-26T11:04:58.086679Z"},"trusted":true},"outputs":[],"source":["#describe train data (price values)\n","train_df.describe()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.105899Z","iopub.status.busy":"2022-03-26T11:04:58.105121Z","iopub.status.idle":"2022-03-26T11:04:58.120263Z","shell.execute_reply":"2022-03-26T11:04:58.119351Z","shell.execute_reply.started":"2022-03-26T11:04:58.105858Z"},"trusted":true},"outputs":[],"source":["#decribe test data\n","test_df.describe()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.122004Z","iopub.status.busy":"2022-03-26T11:04:58.121703Z","iopub.status.idle":"2022-03-26T11:04:58.147930Z","shell.execute_reply":"2022-03-26T11:04:58.146689Z","shell.execute_reply.started":"2022-03-26T11:04:58.121966Z"},"trusted":true},"outputs":[],"source":["print(\"Some information about training dataset.\\n\")\n","print(train_df.info())\n","print(\"\\n============================================\\n\")\n","print(\"Some information about test dataset.\\n\")\n","print(test_df.info())"]},{"cell_type":"markdown","metadata":{},"source":["## preprocessing the data"]},{"cell_type":"markdown","metadata":{},"source":["### remove null values from training dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.149604Z","iopub.status.busy":"2022-03-26T11:04:58.149366Z","iopub.status.idle":"2022-03-26T11:04:58.171128Z","shell.execute_reply":"2022-03-26T11:04:58.170322Z","shell.execute_reply.started":"2022-03-26T11:04:58.149571Z"},"trusted":true},"outputs":[],"source":["print(f\"number of rows in training dataset after dropping all null values {train_df.shape[0]} value\")\n","train_df = train_df.dropna()\n","print(train_df.shape)\n","\n","print(\"Some information about training dataset.\\n\")\n","print(train_df.info())"]},{"cell_type":"markdown","metadata":{},"source":["### check the number of duplicated rows in summary feature and remove them."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.173114Z","iopub.status.busy":"2022-03-26T11:04:58.172635Z","iopub.status.idle":"2022-03-26T11:04:58.199026Z","shell.execute_reply":"2022-03-26T11:04:58.198221Z","shell.execute_reply.started":"2022-03-26T11:04:58.173077Z"},"trusted":true},"outputs":[],"source":["#how many duplicated rows in summary feature\n","print(f\"number of duplicated rows in summary feature in training dataset are {train_df.duplicated(subset='summary',keep='first').sum()} values.\")\n","train_df = train_df.drop_duplicates(subset='summary',keep='first')\n","train_df.reset_index(drop=True, inplace=True)\n","print(f\"number of rows in summary feature in training dataset after dropping duplicated rows is {train_df.shape[0]} value.\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.200721Z","iopub.status.busy":"2022-03-26T11:04:58.200331Z","iopub.status.idle":"2022-03-26T11:04:58.232248Z","shell.execute_reply":"2022-03-26T11:04:58.231515Z","shell.execute_reply.started":"2022-03-26T11:04:58.200686Z"},"trusted":true},"outputs":[],"source":["#check now if how many null values in both datasets\n","print(f\"number of null values in training dataset is {train_df.isnull().sum().sum()} value.\")\n","print(f\"number of null values in testing dataset is {test_df.isnull().sum().sum()} value.\")\n","#check now if how many duplicated values in both datasets\n","print(f\"number of duplicated rows in summary feature in training dataset is {train_df.duplicated(subset='summary',keep='first').sum()} value.\")\n","#there are still duplicated rows in summary feature in testing dataset but we can't remove them.\n","print(f\"number of duplicated rows in summary feature in testing dataset are {test_df.duplicated(subset='summary',keep='first').sum()} values.\")"]},{"cell_type":"markdown","metadata":{},"source":["### translate training summary feature and testing summary feature. "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:04:58.235019Z","iopub.status.busy":"2022-03-26T11:04:58.233294Z","iopub.status.idle":"2022-03-26T11:14:09.729082Z","shell.execute_reply":"2022-03-26T11:14:09.728260Z","shell.execute_reply.started":"2022-03-26T11:04:58.234979Z"},"trusted":true},"outputs":[],"source":["start_time = time()\n","\n","#translate the summary column for both datasets\n","print('Start translating the training dataset')\n","for i in tqdm(range(len(train_df))):\n","    translator = Translator()\n","    train_df.at[i,'summary'] = translator.translate(train_df.at[i,'summary'] , dest='en').text\n","    \n","print(\"Done..\\n\")\n","    \n","print('Start translating testing dataset')\n","for i in tqdm(range(len(test_df))):\n","    translator = Translator()\n","    test_df.at[i,'summary'] = translator.translate(test_df.at[i,'summary'] , dest='en').text\n","print(\"Done..\\n\")\n","\n","\n","\n","print(len(train_df['summary']))\n","print(len(train_df['image']))\n","print(len(train_df['type']))\n","print(len(train_df['price']))\n","test_df['image']\n","\n","end_time = time()\n","total_time = end_time - start_time\n","result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n","print(f\"The total time taken to translate summary feature in both datasets was: {result}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Some data visualization"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:14:09.731140Z","iopub.status.busy":"2022-03-26T11:14:09.730384Z","iopub.status.idle":"2022-03-26T11:14:10.060884Z","shell.execute_reply":"2022-03-26T11:14:10.060181Z","shell.execute_reply.started":"2022-03-26T11:14:09.731102Z"},"trusted":true},"outputs":[],"source":["train_df['type'].value_counts().sort_values(ascending = False).plot(kind='bar')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:14:10.062534Z","iopub.status.busy":"2022-03-26T11:14:10.062156Z","iopub.status.idle":"2022-03-26T11:14:10.231199Z","shell.execute_reply":"2022-03-26T11:14:10.230499Z","shell.execute_reply.started":"2022-03-26T11:14:10.062497Z"},"trusted":true},"outputs":[],"source":["train_df['price'].value_counts().sort_values(ascending = False).plot(kind='bar')"]},{"cell_type":"markdown","metadata":{},"source":["### split training data into training and validation set."]},{"cell_type":"markdown","metadata":{},"source":["### Images data preprocessing\n","Resize each image in image feature and store its value in numpy array (do that for both, training set and test set)."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:14:10.233226Z","iopub.status.busy":"2022-03-26T11:14:10.232710Z","iopub.status.idle":"2022-03-26T11:15:59.001415Z","shell.execute_reply":"2022-03-26T11:15:59.000688Z","shell.execute_reply.started":"2022-03-26T11:14:10.233187Z"},"trusted":true},"outputs":[],"source":["start_time = time()\n","\n","# preprocess image data\n","def load_image(file):\n","    p = pathlib.Path(file)\n","    dir_and_file_name = os.path.join(*p.parts[-2:])\n","    #if you're using kaggle notebook to run this code unhash following line.\n","    file = f\"../input/airbnb/{dir_and_file_name}\"\n","    try:\n","        #read image first\n","        image = cv2.imread(file, 1)\n","        #resize it\n","        image = cv2.resize(image, (64,64), interpolation=cv2.INTER_CUBIC) #read it as rgb to preserve more information about each image.\n","        #store its value\n","        arr = np.array(image)\n","    except:\n","        #if the file doesn't exsit store array of zeros\n","        arr = np.zeros((64, 64, 3))\n","    return arr\n","\n","\n","# loading images:\n","x_train_image = np.array([load_image(i) for i in tqdm(train_df.image)])\n","x_test_image = np.array([load_image(i) for i in tqdm(test_df.image)])\n","\n","# change summary type: \n","train_df.summary = train_df.summary.astype('str')\n","test_df.summary = test_df.summary.astype('str')\n","\n","# get type \n","y_train_type = train_df.type\n","\n","# get price\n","y_train_price = train_df.price\n","end_time = time()\n","total_time = end_time - start_time\n","result = '{0:02.0f} minutes and {1:02.0f} seconds'.format(*divmod((total_time/60) * 60, 60))\n","\n","print(f\"The total time taken to load images from both datasets was: {result}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:15:59.006064Z","iopub.status.busy":"2022-03-26T11:15:59.005626Z","iopub.status.idle":"2022-03-26T11:15:59.014090Z","shell.execute_reply":"2022-03-26T11:15:59.013386Z","shell.execute_reply.started":"2022-03-26T11:15:59.006025Z"},"trusted":true},"outputs":[],"source":["#check if training images loaded correctly\n","print(x_train_image[0])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:15:59.020429Z","iopub.status.busy":"2022-03-26T11:15:59.020031Z","iopub.status.idle":"2022-03-26T11:15:59.028418Z","shell.execute_reply":"2022-03-26T11:15:59.027538Z","shell.execute_reply.started":"2022-03-26T11:15:59.020394Z"},"trusted":true},"outputs":[],"source":["#check if test images loaded correctly\n","x_test_image[0]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:15:59.030205Z","iopub.status.busy":"2022-03-26T11:15:59.029596Z","iopub.status.idle":"2022-03-26T11:15:59.036584Z","shell.execute_reply":"2022-03-26T11:15:59.035701Z","shell.execute_reply.started":"2022-03-26T11:15:59.030169Z"},"trusted":true},"outputs":[],"source":["print(x_train_image.shape)\n","print(x_test_image.shape)\n","print(train_df.summary.shape)\n","print(test_df.summary.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### text data preprocessing\n","\n","tokenize each text in summary feature and then give each token a unique id based on keras tokenizer."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:15:59.038345Z","iopub.status.busy":"2022-03-26T11:15:59.037910Z","iopub.status.idle":"2022-03-26T11:16:00.525981Z","shell.execute_reply":"2022-03-26T11:16:00.525150Z","shell.execute_reply.started":"2022-03-26T11:15:59.038298Z"},"trusted":true},"outputs":[],"source":["vocab_size = 50000\n","max_len = 100\n","\n","\n","# build vocabulary from training set\n","mm_tokenizer = Tokenizer(num_words=vocab_size)\n","mm_tokenizer.fit_on_texts(train_df.summary)\n","mm_tokenizer.fit_on_texts(test_df.summary)\n","\n","\n","def _preprocess(list_of_text):\n","    return pad_sequences(\n","        mm_tokenizer.texts_to_sequences(list_of_text),\n","        maxlen=max_len,\n","        padding='post',\n","    )\n","    \n","\n","# padding is done inside: \n","x_train_text_id = _preprocess(train_df.summary)\n","x_test_text_id = _preprocess(test_df.summary)\n","\n","print(x_train_text_id.shape)\n","print(x_test_text_id.shape)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:00.527844Z","iopub.status.busy":"2022-03-26T11:16:00.527320Z","iopub.status.idle":"2022-03-26T11:16:00.542356Z","shell.execute_reply":"2022-03-26T11:16:00.541584Z","shell.execute_reply.started":"2022-03-26T11:16:00.527791Z"},"trusted":true},"outputs":[],"source":["# we can use the tokenizer to convert IDs to words.\n","pprint(mm_tokenizer.sequences_to_texts(x_train_text_id[:5]))"]},{"cell_type":"markdown","metadata":{},"source":["### text data preprocessing for pretrained model."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:00.544520Z","iopub.status.busy":"2022-03-26T11:16:00.543915Z","iopub.status.idle":"2022-03-26T11:16:01.399854Z","shell.execute_reply":"2022-03-26T11:16:01.398965Z","shell.execute_reply.started":"2022-03-26T11:16:00.544480Z"},"trusted":true},"outputs":[],"source":["## define pretrained tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:01.401980Z","iopub.status.busy":"2022-03-26T11:16:01.401388Z","iopub.status.idle":"2022-03-26T11:16:04.142458Z","shell.execute_reply":"2022-03-26T11:16:04.141704Z","shell.execute_reply.started":"2022-03-26T11:16:01.401943Z"},"trusted":true},"outputs":[],"source":["#check how the value will be after encode it to unique ids\n","token = tokenizer.encode_plus(\n","    train_df['summary'].iloc[611],\n","    max_length=190, \n","    truncation=True, \n","    padding='max_length', \n","    add_special_tokens=True,\n","    return_tensors='tf'\n",")\n","token"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:04.144730Z","iopub.status.busy":"2022-03-26T11:16:04.143879Z","iopub.status.idle":"2022-03-26T11:16:04.150730Z","shell.execute_reply":"2022-03-26T11:16:04.149970Z","shell.execute_reply.started":"2022-03-26T11:16:04.144690Z"},"trusted":true},"outputs":[],"source":["input_ids = np.zeros((len(train_df['summary']), 200))\n","masks = np.zeros((len(train_df['summary']), 200))"]},{"cell_type":"markdown","metadata":{},"source":["### generate_data function\n","\n","- Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n","- Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece…).\n","- Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:04.153040Z","iopub.status.busy":"2022-03-26T11:16:04.152017Z","iopub.status.idle":"2022-03-26T11:16:04.162622Z","shell.execute_reply":"2022-03-26T11:16:04.161784Z","shell.execute_reply.started":"2022-03-26T11:16:04.152993Z"},"trusted":true},"outputs":[],"source":["def generate_data(df, tokenizer,  ids, masks):\n","    for i, text in tqdm(enumerate(df)):\n","        tokenized_text = tokenizer.encode_plus(\n","            text,\n","            max_length=200,  \n","            padding='max_length',\n","            truncation=True,\n","            add_special_tokens=True,\n","            return_tensors='tf'\n","        )\n","        ids[i, :] = tokenized_text.input_ids\n","        masks[i, :] = tokenized_text.attention_mask\n","    return ids, masks"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:04.164961Z","iopub.status.busy":"2022-03-26T11:16:04.163919Z","iopub.status.idle":"2022-03-26T11:16:17.592217Z","shell.execute_reply":"2022-03-26T11:16:17.591383Z","shell.execute_reply.started":"2022-03-26T11:16:04.164921Z"},"trusted":true},"outputs":[],"source":["input_ids, masks = generate_data(train_df['summary'], tokenizer, input_ids, masks)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.594028Z","iopub.status.busy":"2022-03-26T11:16:17.593765Z","iopub.status.idle":"2022-03-26T11:16:17.604006Z","shell.execute_reply":"2022-03-26T11:16:17.603146Z","shell.execute_reply.started":"2022-03-26T11:16:17.593993Z"},"trusted":true},"outputs":[],"source":["labels = np.zeros((len(train_df['price']), 3))\n","\n","print(labels.shape)\n","train_df['price']"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.606083Z","iopub.status.busy":"2022-03-26T11:16:17.605513Z","iopub.status.idle":"2022-03-26T11:16:17.610525Z","shell.execute_reply":"2022-03-26T11:16:17.609840Z","shell.execute_reply.started":"2022-03-26T11:16:17.606044Z"},"trusted":true},"outputs":[],"source":["#convert labels values to one hot encoded data.\n","labels[np.arange(len(train_df['price'])), train_df['price'].values] = 1 # one-hot encoded the label tensor"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.612823Z","iopub.status.busy":"2022-03-26T11:16:17.612255Z","iopub.status.idle":"2022-03-26T11:16:17.622207Z","shell.execute_reply":"2022-03-26T11:16:17.621118Z","shell.execute_reply.started":"2022-03-26T11:16:17.612787Z"},"trusted":true},"outputs":[],"source":["labels"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.624497Z","iopub.status.busy":"2022-03-26T11:16:17.623947Z","iopub.status.idle":"2022-03-26T11:16:17.659338Z","shell.execute_reply":"2022-03-26T11:16:17.658674Z","shell.execute_reply.started":"2022-03-26T11:16:17.624462Z"},"trusted":true},"outputs":[],"source":["#create tensor dataset\n","dataset = tf.data.Dataset.from_tensor_slices((input_ids, masks, labels))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.660959Z","iopub.status.busy":"2022-03-26T11:16:17.660709Z","iopub.status.idle":"2022-03-26T11:16:17.665563Z","shell.execute_reply":"2022-03-26T11:16:17.664907Z","shell.execute_reply.started":"2022-03-26T11:16:17.660926Z"},"trusted":true},"outputs":[],"source":["#function to make a tensor dataset like dataframe with 3 features (2 inputs [ids, mask]) and one output(one hot encoded labels)\n","def dataset_map(input_ids, attention_mask, labels):\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask\n","    }, labels"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.668162Z","iopub.status.busy":"2022-03-26T11:16:17.667463Z","iopub.status.idle":"2022-03-26T11:16:17.720597Z","shell.execute_reply":"2022-03-26T11:16:17.719944Z","shell.execute_reply.started":"2022-03-26T11:16:17.668059Z"},"trusted":true},"outputs":[],"source":["dataset = dataset.map(dataset_map) # converting to required format for tensorflow dataset"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.722174Z","iopub.status.busy":"2022-03-26T11:16:17.721928Z","iopub.status.idle":"2022-03-26T11:16:17.728982Z","shell.execute_reply":"2022-03-26T11:16:17.728260Z","shell.execute_reply.started":"2022-03-26T11:16:17.722139Z"},"trusted":true},"outputs":[],"source":["dataset = dataset.shuffle(10000).batch(32, drop_remainder=True) # batch size, drop any left out tensor"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.731216Z","iopub.status.busy":"2022-03-26T11:16:17.730574Z","iopub.status.idle":"2022-03-26T11:16:17.736512Z","shell.execute_reply":"2022-03-26T11:16:17.735564Z","shell.execute_reply.started":"2022-03-26T11:16:17.731177Z"},"trusted":true},"outputs":[],"source":["batch_train_size = 0.8\n","train_size = int((len(train_df['summary'])//32)*batch_train_size) # for each 32 batch of data we will have len(df)//16 samples, take 80% of that for train."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.738765Z","iopub.status.busy":"2022-03-26T11:16:17.738180Z","iopub.status.idle":"2022-03-26T11:16:17.747582Z","shell.execute_reply":"2022-03-26T11:16:17.746713Z","shell.execute_reply.started":"2022-03-26T11:16:17.738730Z"},"trusted":true},"outputs":[],"source":["#creat training dataset\n","training_dataset = dataset.take(train_size)\n","validation_dataset = dataset.skip(train_size)\n","print(type(training_dataset))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.750030Z","iopub.status.busy":"2022-03-26T11:16:17.749335Z","iopub.status.idle":"2022-03-26T11:16:17.761760Z","shell.execute_reply":"2022-03-26T11:16:17.761003Z","shell.execute_reply.started":"2022-03-26T11:16:17.749992Z"},"trusted":true},"outputs":[],"source":["def plot_performance(history, model_number, training_metrics, validation_metrics, target):\n","    \"\"\"\n","    function to plot training price loss vs validation price loss and training price accuracy vs validation price accuracy.<br>\n","    \n","    params:\n","    \n","    history: model.fit object\n","    model number (int): just integer number to indicate the model number.\n","    training_metrics (list) (2 values): first values like 'loss' will passed to history object\n","    validation_metrics (list) (2 values): first values like 'val_loss' will passed to history object\n","    target (string): just a string to diplay it on the plot set it either price or type.\n","    \"\"\"\n","    val_loss_per_epoch = history.history[validation_metrics[0]]\n","    loss_per_epoch = history.history[training_metrics[0]]\n","    val_accuracy_per_epoch = history.history[validation_metrics[1]]\n","    accuracy_per_epoch = history.history[training_metrics[1]]\n","    plt.figure(figsize=(8,8))\n","    plt.title(f\"model number {model_number} {target} training loos & {target} validation loss with batch size 64\")\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss function')\n","    plt.plot(np.arange(1,len(val_loss_per_epoch)+1),val_loss_per_epoch,label=f\"validation loss\")\n","    plt.plot(np.arange(1,len(loss_per_epoch)+1),loss_per_epoch,label = f\"training loss\")\n","    plt.legend(loc=\"upper left\")\n","    plt.show()\n","    plt.figure(figsize=(8,8))\n","    plt.title(f\"model number {model_number} {target} training accuracy & {target} validation accuracy with batch size 64\")\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.plot(np.arange(1,len(val_accuracy_per_epoch)+1),val_accuracy_per_epoch,label=\"validation accuracy\")\n","    plt.plot(np.arange(1,len(accuracy_per_epoch)+1),accuracy_per_epoch,label = \"training accuracy\")\n","    plt.legend(loc=\"upper left\")\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### First trail:\n","\n","Here, I used a simple **multi-modality** and **multi-Task** model like in the lab to predict the type and the price of the real estate, the model has embedded layer to handel text data and one convolution layer and max-pooling layer to handel image data, then 2 fully connected neural network in parallel to make the model learn more and to see if the model will learn if the number of learnable parameters increased or it'll overfit. *there's a dropout layer between each fully connected layer*, the optimizer here will be **nesterov adam** because this **optimizer** have nesterov momentum unlike adam optimizer which use momentum, so it'll converge faster than adam optimizer.<br>\n","\n","**5,246,459 learnable parameters**"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:17.762856Z","iopub.status.busy":"2022-03-26T11:16:17.762642Z","iopub.status.idle":"2022-03-26T11:16:18.128826Z","shell.execute_reply":"2022-03-26T11:16:18.127972Z","shell.execute_reply.started":"2022-03-26T11:16:17.762828Z"},"trusted":true},"outputs":[],"source":["#first output shape\n","len_type = len(train_df.type.unique())\n","#second output shape\n","len_price = len(train_df.price.unique())\n","train_df['type'] = train_df.type.astype('category').cat.codes\n","\n","# here we have two inputs. one for image and the other for text.\n","in_text = keras.Input(batch_shape=(None, max_len))\n","in_image = keras.Input(batch_shape=(None, 64, 64, 3))\n","\n","\n","#print(reshape.dtype)\n","\n","keras.backend.clear_session()\n","\n","\n","# text part\n","# simple average of embedding.\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","averaged = tf.reduce_mean(embedded, axis=1)\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (16, 16))(in_image)\n","pl = MaxPool2D((16, 16))(cov)\n","flattened = Flatten()(pl)\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","fused2 = Dense(256, activation='relu')(fused)\n","dropout_2 = Dropout(rate = 0.3)(fused2)\n","fused2 =  Dense(128, activation='relu')(dropout_2)\n","\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","p_price = Dense(len_price, activation='softmax', name='price')(fused2)\n","\n","\n","# define model input/output using keys.\n","first_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","first_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type': 'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type': 0.5,\n","        'price': 0.5,       \n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","first_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(first_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start training the first model\n","\n","I'll train the model with 30 epochs and 64 batch size and 20% of the total data as validation data, also I passed early stopping callback to the model to make the model stop training if the price validation accuracy decreased for three times."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:18.130473Z","iopub.status.busy":"2022-03-26T11:16:18.130175Z","iopub.status.idle":"2022-03-26T11:16:31.750067Z","shell.execute_reply":"2022-03-26T11:16:31.749298Z","shell.execute_reply.started":"2022-03-26T11:16:18.130436Z"},"trusted":true},"outputs":[],"source":["history_1 = first_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:31.752121Z","iopub.status.busy":"2022-03-26T11:16:31.751860Z","iopub.status.idle":"2022-03-26T11:16:32.776222Z","shell.execute_reply":"2022-03-26T11:16:32.775541Z","shell.execute_reply.started":"2022-03-26T11:16:31.752085Z"},"trusted":true},"outputs":[],"source":["#plot price performance for first model\n","plot_performance(history_1, 1, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for first model\n","plot_performance(history_1, 1, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using first model."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:32.778211Z","iopub.status.busy":"2022-03-26T11:16:32.777533Z","iopub.status.idle":"2022-03-26T11:16:34.288365Z","shell.execute_reply":"2022-03-26T11:16:34.287526Z","shell.execute_reply.started":"2022-03-26T11:16:32.778168Z"},"trusted":true},"outputs":[],"source":["#start predicting the values of type and price using summary and image as inputs\n","first_prediction = first_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","first_prediction = first_prediction['price']\n","print(first_prediction)\n","\n","# categories\n","first_prediction = np.argmax(first_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using first model."]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:34.290526Z","iopub.status.busy":"2022-03-26T11:16:34.289729Z","iopub.status.idle":"2022-03-26T11:16:34.316976Z","shell.execute_reply":"2022-03-26T11:16:34.316239Z","shell.execute_reply.started":"2022-03-26T11:16:34.290485Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': first_prediction}\n",").to_csv('sample_submission_1.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### first trial result:\n","\n","Amoung all next models, this model beat models which more complex than this simple model, it got score on kaggle (0.62934), and I think because it's the simplest **multi-modality** and **multi-Task** model, and the model didn't overfit on the training dataset."]},{"cell_type":"markdown","metadata":{},"source":["### Second Trial\n","\n","Here I used a similar model to that in first trail with the same objective and more imporved structure but to imporve the accuracy of the model I added a LSTM layer to between the embedded layer and averaged layer,because in terms of memory. Having a good hold over memorizing certain patterns LSTMs and it is effective in memorizing important information.\n","so it will perform fairly better, also I added a convolution layer after the max-pooling layer to extract more features in images and one dropout layer with 0.2 rate between the first convolution layer to make the model more robust to overfitting, with same optimizer in the first trial.<br>\n","\n","**5,380,219 trainable parameters**\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:34.335804Z","iopub.status.busy":"2022-03-26T11:16:34.333061Z","iopub.status.idle":"2022-03-26T11:16:35.140400Z","shell.execute_reply":"2022-03-26T11:16:35.139573Z","shell.execute_reply.started":"2022-03-26T11:16:34.335764Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","\n","# text part\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","lstm = LSTM(units = 128,return_sequences=True)(embedded)\n","averaged = tf.reduce_mean(lstm, axis=1)\n","print(averaged)\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (7, 7))(in_image)\n","dropout_cov = Dropout(rate=0.2)(cov)\n","pl = MaxPool2D((8, 8))(dropout_cov)\n","cov_2 = Conv2D(32, (5, 5))(pl)\n","\n","flattened = Flatten()(cov_2)\n","\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","fused2 = Dense(256, activation='relu')(fused)\n","dropout_2 = Dropout(rate = 0.3)(fused2)\n","fused2 =  Dense(128, activation='relu')(dropout_2)\n","\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","p_price = Dense(len_price, activation='softmax', name='price')(fused2)\n","\n","\n","# define model input/output using keys.\n","second_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","second_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type': 'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type': 0.5,\n","        'price': 0.5,       \n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","second_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(second_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Start training the second model\n","\n","same as the first trail."]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:35.142934Z","iopub.status.busy":"2022-03-26T11:16:35.142184Z","iopub.status.idle":"2022-03-26T11:16:48.778622Z","shell.execute_reply":"2022-03-26T11:16:48.777873Z","shell.execute_reply.started":"2022-03-26T11:16:35.142889Z"},"trusted":true},"outputs":[],"source":["history_2 = second_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:48.780741Z","iopub.status.busy":"2022-03-26T11:16:48.780462Z","iopub.status.idle":"2022-03-26T11:16:59.601523Z","shell.execute_reply":"2022-03-26T11:16:59.600571Z","shell.execute_reply.started":"2022-03-26T11:16:48.780704Z"},"trusted":true},"outputs":[],"source":["#plot price performance for second model\n","plot_performance(history_2, 2, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for second model\n","plot_performance(history_2, 2, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using second model."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:16:59.603434Z","iopub.status.busy":"2022-03-26T11:16:59.602995Z","iopub.status.idle":"2022-03-26T11:17:02.202005Z","shell.execute_reply":"2022-03-26T11:17:02.200720Z","shell.execute_reply.started":"2022-03-26T11:16:59.603396Z"},"trusted":true},"outputs":[],"source":["second_prediction = second_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","second_prediction = second_prediction['price']\n","print(second_prediction)\n","\n","# categories\n","second_prediction = np.argmax(second_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using second model."]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:02.203950Z","iopub.status.busy":"2022-03-26T11:17:02.203563Z","iopub.status.idle":"2022-03-26T11:17:02.225954Z","shell.execute_reply":"2022-03-26T11:17:02.225279Z","shell.execute_reply.started":"2022-03-26T11:17:02.203910Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': second_prediction}\n",").to_csv('sample_submission_2.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### second trial result:\n","\n","this model is a little bit simillar to the previous one, it got a little bit lower score on kaggle (0.62826), and I think that's because I added LSTM layer and make the image classifier layer more complex, lstm layer here could handel vanishing gradient and gradient explosion problems."]},{"cell_type":"markdown","metadata":{},"source":["### Third trial\n","\n","Here I used the same model in the second trail with the same objective but to make a model more effecient in terms of computaion I replaced the LSTM layer with GRU layer between the embedded layer and averaged layer, I used it for the same reason that I used LSTM layer *(explained in the second trial)*, but GRU layer is more effecient in terms of computation and maybe give me a good accuracy since it's has two gates **(update gate and reset gate)**.<br>\n","\n","**5,351,291 trainable parameters** "]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:02.227769Z","iopub.status.busy":"2022-03-26T11:17:02.227326Z","iopub.status.idle":"2022-03-26T11:17:02.827396Z","shell.execute_reply":"2022-03-26T11:17:02.826522Z","shell.execute_reply.started":"2022-03-26T11:17:02.227705Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","# here we have two inputs. one for image and the other for text.\n","\n","# text part\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","gru = GRU(units = 128,return_sequences=True)(embedded)\n","averaged = tf.reduce_mean(gru, axis=1)\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (7, 7))(in_image)\n","dropout_cov = Dropout(rate=0.2)(cov)\n","pl = MaxPool2D((8, 8))(dropout_cov)\n","cov_2 = Conv2D(32, (5, 5))(pl)\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","fused2 = Dense(256, activation='relu')(fused)\n","dropout_2 = Dropout(rate = 0.3)(fused2)\n","fused2 =  Dense(128, activation='relu')(dropout_2)\n","\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","p_price = Dense(len_price, activation='softmax', name='price')(fused2)\n","\n","\n","# define model input/output using keys.\n","third_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","third_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type': 'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type': 0.5,\n","        'price': 0.5,       \n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","third_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(third_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Start training the third model\n","\n","same as the first trail."]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:02.829956Z","iopub.status.busy":"2022-03-26T11:17:02.829226Z","iopub.status.idle":"2022-03-26T11:17:16.199585Z","shell.execute_reply":"2022-03-26T11:17:16.198769Z","shell.execute_reply.started":"2022-03-26T11:17:02.829917Z"},"trusted":true},"outputs":[],"source":["history_3 = third_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:16.201789Z","iopub.status.busy":"2022-03-26T11:17:16.201517Z","iopub.status.idle":"2022-03-26T11:17:16.982567Z","shell.execute_reply":"2022-03-26T11:17:16.981840Z","shell.execute_reply.started":"2022-03-26T11:17:16.201752Z"},"trusted":true},"outputs":[],"source":["#plot price performance for third model\n","plot_performance(history_3, 3, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for third model\n","plot_performance(history_3, 3, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using third model."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:16.984133Z","iopub.status.busy":"2022-03-26T11:17:16.983721Z","iopub.status.idle":"2022-03-26T11:17:19.299704Z","shell.execute_reply":"2022-03-26T11:17:19.298912Z","shell.execute_reply.started":"2022-03-26T11:17:16.984090Z"},"trusted":true},"outputs":[],"source":["third_prediction = third_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","third_prediction = third_prediction['price']\n","print(third_prediction)\n","\n","# categories\n","third_prediction = np.argmax(third_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using third model."]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:19.301480Z","iopub.status.busy":"2022-03-26T11:17:19.301084Z","iopub.status.idle":"2022-03-26T11:17:19.320318Z","shell.execute_reply":"2022-03-26T11:17:19.319663Z","shell.execute_reply.started":"2022-03-26T11:17:19.301442Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': third_prediction}\n",").to_csv('sample_submission_3.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### third trial result:\n","\n","this model faild in predicting price values, it got score on kaggle (0.33423), I think GRU layer here didn't help the model and it suffered from gradient explosion."]},{"cell_type":"markdown","metadata":{},"source":["## Fourh trail:\n","\n","Same as second trial but making lstm layer bidirectional in order to improve the accuracy, bidirectional lstm will memorize the sequence backward (from the future to the past) and forward (from the past to the future), and I think that will imporve the accuracy of this model. <br>\n","\n","**5,546,619 trainable parameters**"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:19.322089Z","iopub.status.busy":"2022-03-26T11:17:19.321794Z","iopub.status.idle":"2022-03-26T11:17:20.165384Z","shell.execute_reply":"2022-03-26T11:17:20.164525Z","shell.execute_reply.started":"2022-03-26T11:17:19.322051Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","\n","# text part\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","bi_lstm = Bidirectional(LSTM(units = 128,return_sequences=True))(embedded)\n","averaged = tf.reduce_mean(bi_lstm, axis=1)\n","\n","\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (7, 7))(in_image)\n","dropout_cov = Dropout(rate=0.2)(cov)\n","pl = MaxPool2D((8, 8))(dropout_cov)\n","cov_2 = Conv2D(32, (5, 5))(pl)\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","fused2 = Dense(256, activation='relu')(fused)\n","dropout_2 = Dropout(rate = 0.3)(fused2)\n","fused2 =  Dense(128, activation='relu')(dropout_2)\n","\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","p_price = Dense(len_price, activation='softmax', name='price')(fused2)\n","\n","\n","# define model input/output using keys.\n","fourth_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","fourth_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type': 'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type': 0.5,\n","        'price': 0.5,       \n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","fourth_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(fourth_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start training the fourth model"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:20.167811Z","iopub.status.busy":"2022-03-26T11:17:20.167348Z","iopub.status.idle":"2022-03-26T11:17:51.612842Z","shell.execute_reply":"2022-03-26T11:17:51.612111Z","shell.execute_reply.started":"2022-03-26T11:17:20.167776Z"},"trusted":true},"outputs":[],"source":["history_4 = fourth_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:17:51.614826Z","iopub.status.busy":"2022-03-26T11:17:51.614530Z","iopub.status.idle":"2022-03-26T11:18:06.398090Z","shell.execute_reply":"2022-03-26T11:18:06.397088Z","shell.execute_reply.started":"2022-03-26T11:17:51.614789Z"},"trusted":true},"outputs":[],"source":["#plot price performance for fourth model\n","plot_performance(history_4, 4, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for fourth model\n","plot_performance(history_4, 4, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using fourth model."]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:06.399749Z","iopub.status.busy":"2022-03-26T11:18:06.399411Z","iopub.status.idle":"2022-03-26T11:18:08.907410Z","shell.execute_reply":"2022-03-26T11:18:08.906526Z","shell.execute_reply.started":"2022-03-26T11:18:06.399711Z"},"trusted":true},"outputs":[],"source":["fourth_prediction = fourth_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","fourth_prediction = fourth_prediction['price']\n","print(fourth_prediction)\n","\n","# categories\n","fourth_prediction = np.argmax(fourth_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using fourth model."]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:08.909607Z","iopub.status.busy":"2022-03-26T11:18:08.908928Z","iopub.status.idle":"2022-03-26T11:18:08.930157Z","shell.execute_reply":"2022-03-26T11:18:08.929419Z","shell.execute_reply.started":"2022-03-26T11:18:08.909563Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': fourth_prediction}\n",").to_csv('sample_submission_4.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### fourth trial result:\n","\n","this model is the worest model amoung all trial, it got higher score on kaggle (0.30353), and I think because I took into considration the time flow because I made LSTM layer biderctional, and that didn't help the model, you can see from the plot that the validation accuracy kept decreased and that's maybe gradient explosion."]},{"cell_type":"markdown","metadata":{},"source":["### first model result:\n","\n","on kaggle it got lowset accuracy amoung these models and, I think becuase it's the simplest "]},{"cell_type":"markdown","metadata":{},"source":["## Fifth trail\n","\n","Same as the third trial but making **GRU** layer bidirectional in order to improve the accuracy, bidirectional gru will memorize the sequence backward (from the future to the past) and forward (from the past to the future), and I think that will imporve the accuracy of this model, and it'll be more efficient in terms of computation.\n","\n","**5,483,003 trainable parameters**"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:08.931984Z","iopub.status.busy":"2022-03-26T11:18:08.931563Z","iopub.status.idle":"2022-03-26T11:18:09.740834Z","shell.execute_reply":"2022-03-26T11:18:09.739929Z","shell.execute_reply.started":"2022-03-26T11:18:08.931945Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","# here we have two inputs. one for image and the other for text.\n","\n","\n","# text part\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","bi_gru =  Bidirectional(GRU(units = 128,return_sequences=True))(embedded)\n","averaged = tf.reduce_mean(bi_gru, axis=1)\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (16, 16))(in_image)\n","pl = MaxPool2D((16, 16))(cov)\n","flattened = Flatten()(pl)\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","fused2 = Dense(256, activation='relu')(fused)\n","dropout_2 = Dropout(rate = 0.3)(fused2)\n","fused2 =  Dense(128, activation='relu')(dropout_2)\n","\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","p_price = Dense(len_price, activation='softmax', name='price')(fused2)\n","\n","\n","# define model input/output using keys.\n","fifth_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","fifth_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type': 'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type': 0.5,\n","        'price': 0.5,       \n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","fifth_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(fifth_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start training the fifth model"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:09.743394Z","iopub.status.busy":"2022-03-26T11:18:09.742561Z","iopub.status.idle":"2022-03-26T11:18:36.776390Z","shell.execute_reply":"2022-03-26T11:18:36.775579Z","shell.execute_reply.started":"2022-03-26T11:18:09.743350Z"},"trusted":true},"outputs":[],"source":["history_5 = fifth_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:36.778204Z","iopub.status.busy":"2022-03-26T11:18:36.777872Z","iopub.status.idle":"2022-03-26T11:18:37.541199Z","shell.execute_reply":"2022-03-26T11:18:37.540515Z","shell.execute_reply.started":"2022-03-26T11:18:36.778165Z"},"trusted":true},"outputs":[],"source":["#plot price performance for fifth model\n","plot_performance(history_5, 5, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for fifth model\n","plot_performance(history_5, 5, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using fifth model."]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:37.543114Z","iopub.status.busy":"2022-03-26T11:18:37.542467Z","iopub.status.idle":"2022-03-26T11:18:41.327870Z","shell.execute_reply":"2022-03-26T11:18:41.327080Z","shell.execute_reply.started":"2022-03-26T11:18:37.543070Z"},"trusted":true},"outputs":[],"source":["fifth_prediction = fifth_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","fifth_prediction = fifth_prediction['price']\n","print(fifth_prediction)\n","\n","# categories\n","fifth_prediction = np.argmax(fifth_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using fifth model."]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:41.329600Z","iopub.status.busy":"2022-03-26T11:18:41.329069Z","iopub.status.idle":"2022-03-26T11:18:41.348553Z","shell.execute_reply":"2022-03-26T11:18:41.347904Z","shell.execute_reply.started":"2022-03-26T11:18:41.329560Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': fifth_prediction}\n",").to_csv('sample_submission_5.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### fifth trial result:\n","\n","this model is better than previous one in score, it got higher score on kaggle (0.62826), because I made GRU biderctional,which made the accuracy better."]},{"cell_type":"markdown","metadata":{},"source":["## Sixth model\n","\n","Here, I built a simple **multi-modality** and **single-task** model like fifth model to predict the price of the real estate, the model has the same structe as fifth model in terms of input and hidden layers but it has only one output since it's single-task model, and I built this model to see if it will perform same as the fifth model or the result will change. <br>\n","\n","**5,304,803 trainable parameters**"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:41.350152Z","iopub.status.busy":"2022-03-26T11:18:41.349825Z","iopub.status.idle":"2022-03-26T11:18:42.108584Z","shell.execute_reply":"2022-03-26T11:18:42.107737Z","shell.execute_reply.started":"2022-03-26T11:18:41.350113Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","# here we have two inputs. one for image and the other for text.\n","in_text = keras.Input(batch_shape=(None, max_len))\n","in_image = keras.Input(batch_shape=(None, 64, 64, 3))\n","\n","#reshape = tf.keras.layers.Reshape((10, 10), input_shape=(None, max_len))(in_text)\n","\n","#print(reshape.dtype)\n","\n","keras.backend.clear_session()\n","\n","\n","# text part\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","bi_gru =  Bidirectional(GRU(units = 128,return_sequences=True))(embedded)\n","averaged = tf.reduce_mean(bi_gru, axis=1)\n","\n","# image part \n","# simple conv2d. you can change it to anything else as needed\n","cov = Conv2D(32, (16, 16))(in_image)\n","pl = MaxPool2D((16, 16))(cov)\n","flattened = Flatten()(pl)\n","\n","# fusion - combinig both\n","fused = tf.concat([averaged , flattened], axis=-1)\n","\n","# single-task learning\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(fused)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","\n","p_price = Dense(len_price, activation='softmax', name='price')(fused1)\n","\n","\n","# define model input/output using keys.\n","sixth_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","        'image': in_image\n","    },\n","    outputs={\n","        'price': p_price,\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for task, loss \n","# weights for the task.\n","sixth_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'price': 1,       \n","    },\n","    metrics={\n","        'price': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","\n","sixth_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(sixth_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start training the sixth model"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:18:42.111028Z","iopub.status.busy":"2022-03-26T11:18:42.110279Z","iopub.status.idle":"2022-03-26T11:19:05.706363Z","shell.execute_reply":"2022-03-26T11:19:05.705626Z","shell.execute_reply.started":"2022-03-26T11:18:42.110991Z"},"trusted":true},"outputs":[],"source":["history_6 = sixth_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","        'image': x_train_image\n","    },\n","    y={\n","        'price': train_df.price,\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:05.708360Z","iopub.status.busy":"2022-03-26T11:19:05.708095Z","iopub.status.idle":"2022-03-26T11:19:06.113207Z","shell.execute_reply":"2022-03-26T11:19:06.112514Z","shell.execute_reply.started":"2022-03-26T11:19:05.708324Z"},"trusted":true},"outputs":[],"source":["#plot price performance for sixth model\n","plot_performance(history_6, 6, ['loss','sparse_categorical_accuracy'], ['val_loss','val_sparse_categorical_accuracy'], 'price')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using sixth model."]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:06.115220Z","iopub.status.busy":"2022-03-26T11:19:06.114743Z","iopub.status.idle":"2022-03-26T11:19:08.443068Z","shell.execute_reply":"2022-03-26T11:19:08.442264Z","shell.execute_reply.started":"2022-03-26T11:19:06.115177Z"},"trusted":true},"outputs":[],"source":["sixth_prediction = sixth_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","        'image': x_test_image\n","    }\n",")\n","\n","\n","# probabilities\n","sixth_prediction = sixth_prediction['price']\n","print(sixth_prediction)\n","\n","# categories\n","sixth_prediction = np.argmax(sixth_prediction, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using sixth model."]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:08.445256Z","iopub.status.busy":"2022-03-26T11:19:08.444414Z","iopub.status.idle":"2022-03-26T11:19:08.464182Z","shell.execute_reply":"2022-03-26T11:19:08.463509Z","shell.execute_reply.started":"2022-03-26T11:19:08.445216Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': sixth_prediction}\n",").to_csv('sample_submission_6.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### sixth trial result:\n","\n","this model is built with bi-derctional GUR layer to memorize text sequence, it got score on kaggle (0.62934), and I noticed that the model stop learning at some point, but it's still better a little bit than previous model."]},{"cell_type":"markdown","metadata":{},"source":["## Last trail:\n","\n","I built a simple **single-modality** and **multi-task** model to predict the type and the price of real estate, it has an embedded layer -> bidrectional lstm layer to memorize the patterns in both directions -> averaged layer -> 2 fully connected layer to extract more features and imporve the learning process -> 2 output layers with softmax activation function one for type prediction and the other for price prediction. <br>\n","\n","**5,307,355 trainable parameters**"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:08.465737Z","iopub.status.busy":"2022-03-26T11:19:08.465411Z","iopub.status.idle":"2022-03-26T11:19:09.678350Z","shell.execute_reply":"2022-03-26T11:19:09.677536Z","shell.execute_reply.started":"2022-03-26T11:19:08.465701Z"},"trusted":true},"outputs":[],"source":["keras.backend.clear_session()\n","\n","# simple average of embedding. you can change it to anything else as needed\n","\n","embedded = keras.layers.Embedding(mm_tokenizer.num_words, 100)(in_text)\n","bi_lstm =  Bidirectional(LSTM(units = 128,return_sequences=True))(embedded)\n","averaged = tf.reduce_mean(bi_lstm, axis=1)\n","\n","\n","# multi-task learning (each is a multi-class classification)\n","# one dense layer for each task\n","\n","fused1 = Dense(128, activation='relu')(averaged)\n","dropout_1 = Dropout(rate = 0.2)(fused1)\n","fused1 = Dense(256, activation='relu')(dropout_1)\n","\n","p_price = Dense(len_price, activation='softmax', name='price')(fused1)\n","p_type = Dense(len_type, activation='softmax', name='type')(fused1)\n","\n","\n","# define model input/output using keys.\n","seventh_model = keras.Model(\n","    inputs={\n","        'summary': in_text,\n","    },\n","    outputs={\n","        'type': p_type,\n","        'price': p_price,\n","\n","    },\n",")\n","\n","\n","# compile model with optimizer, loss values for each task, loss \n","# weights for each task.\n","seventh_model.compile(\n","    optimizer=Nadam(),\n","    loss={\n","        'type':'sparse_categorical_crossentropy',\n","        'price': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'type':0.5,\n","        'price': 0.5\n","    },\n","    metrics={\n","        'type': ['SparseCategoricalAccuracy'],\n","        'price': ['SparseCategoricalAccuracy']\n","    },\n",")\n","\n","\n","seventh_model.summary()\n","#plot the model\n","tf.keras.utils.plot_model(seventh_model, show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start training the last model"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:09.681026Z","iopub.status.busy":"2022-03-26T11:19:09.680479Z","iopub.status.idle":"2022-03-26T11:19:27.414576Z","shell.execute_reply":"2022-03-26T11:19:27.413858Z","shell.execute_reply.started":"2022-03-26T11:19:09.680989Z"},"trusted":true},"outputs":[],"source":["history_7 = seventh_model.fit(\n","    x={\n","        'summary': x_train_text_id,\n","    },\n","\n","    y={\n","        'type': train_df.type,\n","        'price': train_df.price\n","    },\n","    epochs=30,\n","    batch_size=64,\n","    validation_split = 0.2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_sparse_categorical_accuracy', patience=5, )],\n","    verbose=1\n",")"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:27.416608Z","iopub.status.busy":"2022-03-26T11:19:27.416027Z","iopub.status.idle":"2022-03-26T11:19:34.579079Z","shell.execute_reply":"2022-03-26T11:19:34.578340Z","shell.execute_reply.started":"2022-03-26T11:19:27.416565Z"},"trusted":true},"outputs":[],"source":["#plot price performance for last model\n","plot_performance(history_7, 7, ['price_loss','price_sparse_categorical_accuracy'], ['val_price_loss','val_price_sparse_categorical_accuracy'], 'price')\n","#plot type performance for last model\n","plot_performance(history_7, 7, ['type_loss','type_sparse_categorical_accuracy'], ['val_type_loss','val_type_sparse_categorical_accuracy'], 'type')"]},{"cell_type":"markdown","metadata":{},"source":["### start predicting testing price values using seventh model."]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:34.586726Z","iopub.status.busy":"2022-03-26T11:19:34.583568Z","iopub.status.idle":"2022-03-26T11:19:36.456369Z","shell.execute_reply":"2022-03-26T11:19:36.455566Z","shell.execute_reply.started":"2022-03-26T11:19:34.586683Z"},"trusted":true},"outputs":[],"source":["seventh_prediction = seventh_model.predict(\n","    {\n","        'summary': x_test_text_id,\n","    }\n",")\n","\n","\n","# probabilities\n","seventh_prediction = seventh_prediction['price']\n","print(seventh_prediction)\n","\n","# categories\n","seventh_prediction = np.argmax(seventh_prediction, axis=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle using seventh model."]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:36.458281Z","iopub.status.busy":"2022-03-26T11:19:36.458028Z","iopub.status.idle":"2022-03-26T11:19:36.477109Z","shell.execute_reply":"2022-03-26T11:19:36.476421Z","shell.execute_reply.started":"2022-03-26T11:19:36.458246Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': seventh_prediction}\n",").to_csv('sample_submission_7.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### last trial result:\n","\n","Actually this model got lower accuracy than **multi-modality and single-tasking** model and it is not good but still better than some trials here, because from one input it can't get good result for two outputs, kaggle score (0.61494)."]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained model (Bert) - bouns section\n","\n","Here, I decided to use a complex pretrained model and make it train on my text data only and predict only the price values, because bert model only learn on text data not images and then I added a hidden layer and the output layers with 3 units.<br>\n","\n","**108,705,539 trainable parameters**"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:19:36.478484Z","iopub.status.busy":"2022-03-26T11:19:36.478215Z","iopub.status.idle":"2022-03-26T11:19:37.700112Z","shell.execute_reply":"2022-03-26T11:19:37.699363Z","shell.execute_reply.started":"2022-03-26T11:19:36.478449Z"},"trusted":true},"outputs":[],"source":["pretrained_model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:22:00.447356Z","iopub.status.busy":"2022-03-26T11:22:00.446634Z","iopub.status.idle":"2022-03-26T11:22:03.036605Z","shell.execute_reply":"2022-03-26T11:22:03.035595Z","shell.execute_reply.started":"2022-03-26T11:22:00.447311Z"},"trusted":true},"outputs":[],"source":["input_ids = tf.keras.layers.Input(shape=(200,), name='input_ids', dtype='int32')\n","attention_mask = tf.keras.layers.Input(shape=(200,), name='attention_mask', dtype='int32')\n","\n","bert_embds = pretrained_model.bert(input_ids, attention_mask=attention_mask)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n","hidden_layer = tf.keras.layers.Dense(320, activation='relu', name='hidden_layer')(bert_embds)\n","output_layer = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(hidden_layer) # softmax activation function -> calculates probabilities of classes\n","\n","bert_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n","bert_model.summary()\n","tf.keras.utils.plot_model(bert_model, show_shapes=True)"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:22:03.039592Z","iopub.status.busy":"2022-03-26T11:22:03.039299Z","iopub.status.idle":"2022-03-26T11:22:03.060596Z","shell.execute_reply":"2022-03-26T11:22:03.059938Z","shell.execute_reply.started":"2022-03-26T11:22:03.039554Z"},"trusted":true},"outputs":[],"source":["#learning rate with learning rate scheduler\n","opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n","#metrics is accuracy here because I encoded the price target to one-hot-encoded data.\n","bert_model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy('accuracy')])"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T11:22:08.682438Z","iopub.status.busy":"2022-03-26T11:22:08.682164Z","iopub.status.idle":"2022-03-26T12:14:48.223722Z","shell.execute_reply":"2022-03-26T12:14:48.222982Z","shell.execute_reply.started":"2022-03-26T11:22:08.682407Z"},"trusted":true},"outputs":[],"source":["tf.debugging.set_log_device_placement(True)\n","hist = bert_model.fit(\n","    training_dataset,\n","    validation_data=validation_dataset,\n","    epochs=25,\n","    workers=1500,\n","    use_multiprocessing=True,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n","    ],\n",")"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T12:14:48.225914Z","iopub.status.busy":"2022-03-26T12:14:48.225550Z","iopub.status.idle":"2022-03-26T12:14:48.632185Z","shell.execute_reply":"2022-03-26T12:14:48.631486Z","shell.execute_reply.started":"2022-03-26T12:14:48.225871Z"},"trusted":true},"outputs":[],"source":["#plot price performance for last model (the hero)\n","plot_performance(hist, 1, ['loss','accuracy'], ['val_loss','val_accuracy'], 'price')"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T12:14:48.634069Z","iopub.status.busy":"2022-03-26T12:14:48.633587Z","iopub.status.idle":"2022-03-26T12:23:32.491446Z","shell.execute_reply":"2022-03-26T12:23:32.490739Z","shell.execute_reply.started":"2022-03-26T12:14:48.634033Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","\n","#preprocess testing dataset input as we did with training dataset input\n","def prepare_data(test_text, tokenizer):\n","    data = []\n","    for i, text in tqdm(enumerate(test_text)):\n","        token = tokenizer.encode_plus(\n","            test_text.iloc[i],\n","            max_length=200, \n","            truncation=True, \n","            padding='max_length', \n","            add_special_tokens=True,\n","            return_tensors='tf'\n","        )\n","        data.append({\n","        'input_ids': tf.cast(token.input_ids, tf.float64),\n","        'attention_mask': tf.cast(token.attention_mask, tf.float64)\n","        })\n","        \n","    return data\n","\n","#prediction function will predict each text observation using for loop\n","def make_prediction(model, processed_data,classes=[0,1,2]):\n","    prediction_data = []\n","    for i in tqdm(range(len(processed_data))):\n","        prediction =  model.predict(processed_data[i])[0]\n","        prediction_data.append(classes[np.argmax(prediction)])\n","    return prediction_data\n","\n","\n","\n","processed_data = prepare_data(test_df['summary'], tokenizer)\n","\n","prediction_data = make_prediction(bert_model, processed_data)\n","\n","print(len(prediction_data))\n","print(prediction_data)"]},{"cell_type":"markdown","metadata":{},"source":["### submission file for kaggle"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-03-26T12:23:32.498815Z","iopub.status.busy":"2022-03-26T12:23:32.493237Z","iopub.status.idle":"2022-03-26T12:23:32.531857Z","shell.execute_reply":"2022-03-26T12:23:32.531209Z","shell.execute_reply.started":"2022-03-26T12:23:32.498761Z"},"trusted":true},"outputs":[],"source":["pd.DataFrame(\n","    {'id': test_df['id'],\n","     'price': prediction_data}).to_csv('sample_submission_pretrained_model.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
