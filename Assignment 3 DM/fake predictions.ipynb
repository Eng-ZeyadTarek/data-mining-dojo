{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining questions\n",
    "\n",
    "### 1- What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "- **Character n-gram** is a contiguous series of n characters from a given sample of text or speech, whereas **word n-grams** is is a contiguous series of n words from a given sample of text or speech, and the one who will suffer from the **out-of-vocabulary** issue is **word n-gram**\n",
    "<hr>\n",
    "\n",
    "### 2- What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "- Both, Are the most important preprocessing techniques for text, but the main difference is that the *stop word removal* totally remove some predefined words that it knows from the sentence and have a list of these words, in other hand, *stemming* is about to reduce the word and return it to its root, like removing suffix and prefixes not removing the whole word, ex (playing -> play), etc, both are language dependant stop words in English not like in German and vice versa also the grammars in English not like in the German language. \n",
    "\n",
    "<hr>\n",
    "\n",
    "### 3-  Is tokenization techniques language dependent? Why?\n",
    "- No, It divides a chunk of text into distinct words based on a specific delimiter. Different word-level tokens are created depending on the delimiters, not the language.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 4- What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "- **count vectorizer** used to convert a text into a vector-based on the frequency (count) of each word that appears throughout the text, whereas **tf-idf vectorizer** it divides into two parts **TF** which is refer to term frequency where we count how many time that the word appears in the text and then dividing it by the total numbers of words in the whole text, where **IDF** is about taking the logarithm to the previous result to decrease the weight of a common word in the sentence or document, it wouldn't be feasiable and it would be **np-complete** problem,but you can select them by using some of search method techniques like (Grid search, random search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation:\n",
    "\n",
    "our problem here is about building a model to classify and detect which real news and fake news from just its titles, our inputs here are news titles (60001) observations for a training dataset and the output is (60001) label.\n",
    "\n",
    "#### **Note:- you'll find the dataset at https://www.kaggle.com/competitions/cisc-873-dm-f22-a3**\n",
    "\n",
    "### Data mining function:\n",
    "- text preprocessing -> tokenization and vectorization each text -> building and training the model -> classification and prediction.\n",
    "\n",
    "### The challenges:\n",
    "\n",
    "- We have a quite big data set so maybe will take some time to preprocess it and train it.\n",
    "\n",
    "- Our input here is dirty text, each observation here has a lot of punctuation and non-English letters and misspellings, and grammatical mistakes because each title here is typed by humans, so we need to choose a proper text cleaning technique by trying each one them and choose the one who give us best results.\n",
    "\n",
    "- Even if we cleaned our input here, it's still string data and machine learning models can't deal with string data so we need to convert them into numeric data by technique called vectorization.\n",
    "\n",
    "### The impact:\n",
    "- Building this model will solve many social problems and prevent the spread of rumors on social media quickly.\n",
    "\n",
    "### The ideal solution:\n",
    "- a pipeline has **tf-idf vectorizer** and **logistic regression model** and searches for the best hyperparameter combination using random search technique. **(roc_auc score) 0.87006, third place**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental protocol:\n",
    "\n",
    "- Read training data.\n",
    "- Preprocessing:\n",
    "    * cleaning input data for both training set and test set using some regular expressions patterns and stemming or lemmatizing each text.\n",
    "- Data exploration\n",
    "- Data vectorization: There are two famous vectorization techniques **count vectorizer** and **tf-idf vectorizer** we will try each one of them and choose the one who will give us best results.\n",
    "- Building our piplines (has the vectorizer and machine learning model)\n",
    "- Build the search speace and search for the best hyperparameters combinations but trying many fits.\n",
    "- Test our model\n",
    "- Choose the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF: https://onq.queensu.ca/d2l/le/content/626484/viewContent/3870297/View\n",
    "\n",
    "Day 7 > lab 5-6 NLP > 873_nlp.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 100\n",
    "np.set_printoptions(threshold=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59151, 1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('xy_train.csv', sep=\",\", na_values=[\"\"])\n",
    "data = data.drop(data[data.label == 2].index)\n",
    "data.label = data.label.astype('int8')\n",
    "\n",
    "test = pd.read_csv('x_test.csv',index_col='id')\n",
    "id = test.index\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define two methods for text cleaning\n",
    "\n",
    "First method will clean text and will **lemmtize** each text if <code>for_embedding</code> parameter was false.\n",
    "\n",
    "Second method will clean text and will **stem** each text if <code>for_embedding</code> parameter was false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"shan't\", 'of', 'doing', 'between', 'both', 'shouldn', 'under', 'o', 'me', 'over', 'what', 'own', 'or', 're', 'be', 'your', 'with', 'were', 'once', 'an', 'i', 'wouldn', \"hadn't\", \"mustn't\", \"you'll\", 'on', 'aren', \"hasn't\", 'into', 'no', 'should', \"wouldn't\", 'against', 'will', 'out', 'now', 'needn', 'further', 'before', 'so', 'too', 'who', 'you', \"won't\", 'from', 'doesn', 'mightn', 'weren', 'only', 'if', 'ain', 'just', 'do', 'its', 'up', 'all', 'while', 'them', \"don't\", 'having', \"wasn't\", 'there', 'and', 'hadn', 'my', 'herself', 'myself', 'this', \"shouldn't\", 'but', 'have', 'then', 've', 'our', \"she's\", 'himself', 'did', 'such', 'themselves', 's', 'each', 'm', \"you're\", \"weren't\", 'very', 'y', 'again', 'after', 'wasn', 'same', 'hers', 'll', 'his', 'how', 'not', 'yourselves', 'where', 'they', 'theirs', 'that', 'why', 'some', 'haven', 'their', 'other', 'the', \"isn't\", 'being', 'during', 'ma', 'been', \"mightn't\", 'does', 'can', 'above', 'has', \"that'll\", 'these', 'had', 'in', 'yourself', 'at', \"you've\", \"should've\", 'she', 'are', 'more', 'was', 'those', 'couldn', 'him', 'any', 'am', 'd', 'most', \"you'd\", 'yours', 'because', 'below', 'when', 'ours', 'we', 'which', 'until', 'whom', \"it's\", 'for', \"couldn't\", 'he', 'through', 'than', 'ourselves', \"aren't\", 'by', 'to', 'here', 'a', 'mustn', 'nor', 'didn', 'shan', 'down', 'hasn', 'her', 'itself', 'off', 'it', 't', \"didn't\", 'about', \"haven't\", 'few', 'won', \"needn't\", 'don', 'isn', \"doesn't\", 'as', 'is'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "print(stop_words)\n",
    "\n",
    "def RegEx_clean(text, for_embedding):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) #match one or more white sepace\n",
    "    RE_TAGS = re.compile(r\"<.*?>\") #match <any num of words>\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž0-9]+\", re.IGNORECASE) #match any English word\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b^[^A-Za-zÀ-ž0-9]+\\b\", re.IGNORECASE) #match any word with word boundary.\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE) #match any English word and any punctuation\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE) #match any word and any punctuation with word boundary.\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text) #remove one or more white sepace\n",
    "    text = re.sub(RE_ASCII, \" \", text) #remove <any num of words>\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text) #remove any English word\n",
    "    text = re.sub(RE_WSPACE, \" \", text) #remove any word with word boundary.\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "def lemmatize_clean_text(text ,for_embedding=False):\n",
    "\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and lemmatize\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    word_tokens = RegEx_clean(text, for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "def stemming_clean_text(text ,for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemming\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    word_tokens = RegEx_clean(text,for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [stemmer.stem(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start preprocessing text feature in two ways.\n",
    "\n",
    "1- clean text with lemmatizing each word in the text (lemmatizing will remove any word ending with take in consideration the meaning of the word).<br>\n",
    "2- clean text with stemming each word in the text  (stemming will remove any word ending without take in consideration the meaning of the word).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data.loc[data[\"text\"].str.len() > 20, \"text\"]\n",
    "data_lemmatized = data[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## clean and lemmatiz training set\n",
    "data_stemmed = data[\"text\"].map(lambda x: stemming_clean_text(x, for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and stemming training set\n",
    "test_lemmatized = test[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set\n",
    "test_stemmed = test[\"text\"].map(lambda x: stemming_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "see the difference between each preprocessing technique.\n",
    "\n",
    "look at the first sentence **\"group friend began volunteer homeless shelter neighbor protested seeing another person also need...\"**\n",
    "\n",
    "see the words **'protested'** and **'another'** in this sentence after lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    group friend began volunteer homeless shelter neighbor protested seeing another person also need...\n",
       "1    british prime minister theresa may nerve attack former russian spy government concluded highly l...\n",
       "2    1961 goodyear released kit allows ps2s brought heel http youtube com watch v alxulk0t8cg 0 72 0 ...\n",
       "3    happy birthday bob barker price right host like remembered man said ave pet spayed neutered 0 92...\n",
       "4    obama nation innocent cop unarmed young black men dying magic johnson 1 0 0 2 1 jimbobshawobodob...\n",
       "5    1920 hitler forbidden address public meeting much germany major blow nazi poster cartoonist phil...\n",
       "6    nerd win scrabble word never heard braconid meaning ny numerous wasp family braconidae larva whi...\n",
       "7    95 8 female newscaster hair style standard result longstanding requirement female reporter job f...\n",
       "8    donald trump say inappropriate tape come continue talk bill hillary clinton inappropriate thing ...\n",
       "9    5 crazy fact lamborghini outrageous electric supercar tire old built 3 month show concept nanotu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence:  group friend began volunteer homeless shelter neighbor protested seeing another person also need...\n",
    "data_lemmatized.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at the first sentence **\"group friend began volunteer homeless shelter neighbor protested seeing another person also need...\"**\n",
    "\n",
    "see the words **'protested'** and **'another'** in this sentence after stemming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...\n",
       "1      british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...\n",
       "2      1961 goodyear releas kit allow ps2s brought heel https youtub com watch v alxulk0t8cg 0 72 0 2 3...\n",
       "3      happi birthday bob barker price right host like rememb man said ave pet spay neuter 0 92 0 2 5 f...\n",
       "4      obama nation innoc cop unarm young black men die magic johnson 1 0 0 2 1 jimbobshawobodob olymp ...\n",
       "5      1920 hitler forbidden address public meet much germani major blow nazi poster cartoonist philipp...\n",
       "6      nerd win scrabbl word never heard braconid mean ny numer wasp famili braconida larva whichar par...\n",
       "7      95 8 femal newscast hair style standard result longstand requir femal report job fulfil larger a...\n",
       "8      donald trump say inappropri tape come continu talk bill hillari clinton inappropri thing 0 94 0 ...\n",
       "9      5 crazi fact lamborghini outrag electr supercar tire old built 3 month show concept nanotub base...\n",
       "10     berni sander come walmart sharehold meet group unit respect call worker represent board director...\n",
       "11     jame corden reveal would kany west rush stage grammi l take hit floor quicker ever known 0 95 0 ...\n",
       "12     sorri r2 d2 0 2 4 welashubbi scientist discov fart prevent breast cancer 1522678339 0 dailypiffl...\n",
       "13     granddaddi someth everyon take grant solid state lamp led made general electr 1967 press releas ...\n",
       "14     minnesota st paul launch web portal opportun zone cover 20 percent citi websit stpaul gov opport...\n",
       "15     dem rep say particip moment silenc las vega victim orlando join colleagu moment silenc hous floo...\n",
       "16     woman lose everyth apart fire find dog 12 hour later oh god said thought lost 0 93 1 0 0 revenge...\n",
       "17     stori follow white nationalist charlottesvill fire polic ever move 0 75 1 0 0 amandavanhouten mi...\n",
       "18     oregon eyewit blast white domest terror say milit will die 0 69 1 0 0 captainopinion aaauuuuuggh...\n",
       "19     julia parachut 0 2 4 frenchymanfella perri platypus born c 1570306424 0 redd true ddsm7l https p...\n",
       "20     wife left two day ago 0 2 4 derewigewolf88 nation socialist poster 1460291977 0 imgur com true 4...\n",
       "21     love stori day spent maddi stuart model syndrom one disabl 0 64 1 0 0 dom9360 dog play park 1498...\n",
       "22     world first head transplant volunt could experi someth wors death 0 85 1 0 0 guzzler829 suppos h...\n",
       "23     fascist infest 1970 0 97 0 1 5 jodi boo huski rubber duck 1531506899 0 redd true 8ymivm https pr...\n",
       "24     oh mind 0 79 0 2 2 wuppindalsa cooki monster expos nsfw 1416795758 0 imgur com true 2n81wa https...\n",
       "25     terribl ghost 1944 hitler sleep dark skeleton appear goosebump rise skin ememb hitler stalingrad...\n",
       "26     fuck fuck say littl bitch l know graduat top class navi seal e involv numer secret raid al quaed...\n",
       "27     joe budden respons quavo call p said og respect 0 82 0 2 5 teckkg imgur 1355857122 0 true c7imf5...\n",
       "28     behind uckserv slur erick erickson correct observ rememb hear term cuckserv slur christian voter...\n",
       "29     oxford dictionari word year word nobodi actual use youthquak signific cultur polit social chang ...\n",
       "30                                                                                   h p p b r h e r z l g\n",
       "31     fuck fuck say littl bitch l know graduat top class navi seal e involv numer secret raid al quaed...\n",
       "32     woman restaur complain kid respond children charm one author said hope someon take care e old 0 ...\n",
       "33     look around mani thing see would say could never 0 2 4 mnewman19 murica 1365451082 0 true c9b2vv...\n",
       "34     ay chees wooah 0 85 0 2 2 ilikeneuron gop slowli pivot carbon tax 1561474921 0 axio com true c59...\n",
       "35     think import time mexico us relat think import time reach sort hold hand said consul general vig...\n",
       "36     howdi ya name kim john 35 year old korean gunahol weapon enthusiast foreign import various ameri...\n",
       "37     mccain say trump congression republican putin job 0 71 1 0 0 dady977 man tell cop hooker rip 147...\n",
       "38                                                                             h e r e w e h n g f f b u h\n",
       "39     battl long drawn suffer sever loss last ditch effort save littl left stroke luck miracul left us...\n",
       "40     dovakhiin dovakhiin naal ok zin los vahriin wah dein vokul mahfoerook asht vaal ahrk fin norok p...\n",
       "41                                                                         p h n g l u g l w n f h h e c e\n",
       "42     news updat trump disqualifi republican nomin sasha baron cohen charact actor behind borat admit ...\n",
       "43     know know want look keith tell particular set skill gossip skill acquir long time skill make nig...\n",
       "44     warn hous card season 2 spoiler click seen hous card season 2 get seen anyway still care whoever...\n",
       "45     http www googl ca imgr um 1 hl en safe sa n tbo rlz 1c1chfx enca474ca474 biw 1920 bih 993 tbm is...\n",
       "46     martha got home earli work son room usual time forget lock door could hear shout get rest muffl ...\n",
       "47     next item item number 4 5 7 8 1 1 1 look stun earring genuin faux sapphir earring fourteen karat...\n",
       "48     enemi imperium hear come die immort emperor us invinc soldier strike war machin crush tread migh...\n",
       "49     assign take hill four us five count vicent lost hand grenad went fight could first met young bra...\n",
       "50                                                                                     n e w p h n e w h h\n",
       "51     lie fed legend harri want know realli happen thirteen year ago shall divulg truli lost power lov...\n",
       "52     anoth thing got forgotten fact probabl sperm whale sudden call exist sever mile surfac alien pla...\n",
       "53     alright see lion tiger rare indian speci indian eat rare sight see one indian speci alway wait o...\n",
       "54     eventu miss richardson stop call fluffi move back room answer alway wrong caus great disrupt amo...\n",
       "55     unrel quot funni thing hike friend get bit poison snake tell go help go ten feet pretend got bit...\n",
       "56                                                                                           h e r e c n g\n",
       "57     shore see submarin mean bad cod better go way see must beach hope whale time whole binocular thi...\n",
       "58     chang hors jackass chang two pale virgin tan skank put death date liberti prosper leav helmet ga...\n",
       "59     gal pago tortois gal pago giant tortois chelonoidi nigra largest live speci tortois 13th heavies...\n",
       "60     know thing shark got lifeless eye black eye like doll eye come ya seem livin bite ya black eye r...\n",
       "61     resid town say banksi new piec make sens whatsoev 30 someth singl mother live neighborhood banks...\n",
       "62     licens kill gopher govern unit nation man free kill gopher kill must know enemi case enemi varmi...\n",
       "63     music box broken start play haunt tune fill air awak sudden dream music box yet tini one nestl h...\n",
       "64     saturday night live star piec toast two guy handlebar mustach man paint silver make robot nois g...\n",
       "65     littl timmi cancer know film want show twerk skill ya know kept roll happen thought let keep gre...\n",
       "66     son yo got ta eat yo vegga tubbl yo wan na grow beeg stwong wike yo daddi mr denni wodman heww o...\n",
       "67     look know twelv pictur eliev hold ten finger know know know everybodi know peopl know know like ...\n",
       "69     trump administr call perman restor bulk phone communic surveil outgo director nation intellig da...\n",
       "70     mother autist boy get take first birthday parti birthday boy mother said want work around timoth...\n",
       "72     mexican propaganda end wwii artist graphic popular workshop glad join joy mexican worlwid worker...\n",
       "73     colorado hous approv three bill increas access afford hous hb19 1309 creat obil home park disput...\n",
       "74     case communiti colleg riley anderson c student high school graduat went communiti colleg south d...\n",
       "75     guy deepli debt suffer throat cancer rob canadian bank turn express deep remors toward bank tell...\n",
       "76     mani girl pictur subtitl mani girl think photo obvious liter reason even ask question two girl s...\n",
       "77     1 entrepreneur send thousand bike children rural myanmar 2 anim thought extinct rediscov jungl h...\n",
       "78     coupl gave birth beauti twin see parent milk forc model children twice first fail girl becam ups...\n",
       "79     franc start work revolutionari alzheim villag patient roam almost free work begun franc first al...\n",
       "80     front cover kangura magazin depict rwandan presid gr goir kayibanda 1962 1973 first post indepen...\n",
       "81     biggest wind farm southern hemispher built 130 kilometr west melbourn power estim half million h...\n",
       "82     air forc veteran blast liber actor call ban white peopl conserv spin site call mark ruffalo raci...\n",
       "83     four month row fourth tuesday month 20 30 homeless peopl found hot shower safe clean place go ba...\n",
       "84     medic expert say new blood test predict develop potenti dead pregnanc disord pre eclampsia save ...\n",
       "85     shock part donald trump tax record 916 million loss everyon talk 15 818 562 loss report line 11 ...\n",
       "86     divorc mother two donat kidney die boss thing get realli weird match donat kidney someon els mov...\n",
       "87     origin browni origin recepi legend say bertha palmer promin chicago socialit whose husband own p...\n",
       "88     cossack soldier stand protelariat toppl capitalist propaganda fyodor podtyolkov soviet republ ma...\n",
       "89     barcelona car free superblock could save hundr live superblock group street traffic reduc close ...\n",
       "90     go lindsay lohan crown princ saudi arabia suppos know rumor friendship someth lohan want make mo...\n",
       "91     homeopath remedi made saliva rabid dog longer avail health canada say although continu licens si...\n",
       "92     bring togeth young old eas isol rural life group minnesota tackl problem social isol loneli nove...\n",
       "93     1926 plump fellow wine event odd insist show uncommon gold teeth cameraman polit tri indic need ...\n",
       "94     secretari kerri call turkish minist turkish govern offici saturday said would view u enemi obama...\n",
       "95     research report low level thc main psychoact compound marijuana reduc stress high dose depend ma...\n",
       "96     school voucher bill narrowli pass hous senat vote thursday ur student memphi nashvill desper nee...\n",
       "97     herp vaccin test human best result yet anim find new studi herp simplex 2 virus hsv2 experiment ...\n",
       "98     japanes pm visit ann frank hous amsterdam week 300 copi book defac tokyo look ahead mani year 21...\n",
       "99     mum two son avoid potenti abduct thank two word taught phrase tricki person idea kid differenti ...\n",
       "100    china take multibillion yuan oilet revolut nationwid door close three year bathroom blitz touris...\n",
       "101    govern pledg creat 41 new marin conserv zone around britain coastlin block arm activ like dredg ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stemmed.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year         4121\n",
       "one          3285\n",
       "new          2998\n",
       "like         2949\n",
       "man          2705\n",
       "trump        2577\n",
       "u            2513\n",
       "colorized    2430\n",
       "people       2315\n",
       "first        2247\n",
       "old          2222\n",
       "look         2214\n",
       "say          2147\n",
       "get          2072\n",
       "time         2011\n",
       "poster       1999\n",
       "2            1987\n",
       "found        1959\n",
       "day          1934\n",
       "woman        1892\n",
       "war          1858\n",
       "life         1768\n",
       "make         1727\n",
       "1            1697\n",
       "world        1570\n",
       "american     1498\n",
       "psbattle     1468\n",
       "state        1387\n",
       "post         1384\n",
       "two          1364\n",
       "school       1339\n",
       "back         1325\n",
       "photo        1324\n",
       "made         1314\n",
       "right        1301\n",
       "r            1266\n",
       "circa        1249\n",
       "child        1216\n",
       "know         1201\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in data_lemmatized\n",
    "word_freq_lemmatized = pd.Series(\" \".join(data_lemmatized).split()).value_counts()\n",
    "word_freq_lemmatized[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year        4125\n",
       "one         3285\n",
       "like        3128\n",
       "new         2998\n",
       "look        2847\n",
       "color       2737\n",
       "man         2728\n",
       "get         2602\n",
       "trump       2578\n",
       "say         2347\n",
       "peopl       2316\n",
       "use         2307\n",
       "first       2248\n",
       "make        2227\n",
       "old         2226\n",
       "time        2027\n",
       "poster      2000\n",
       "found       1999\n",
       "2           1986\n",
       "day         1934\n",
       "war         1858\n",
       "1           1697\n",
       "post        1648\n",
       "world       1570\n",
       "work        1531\n",
       "show        1513\n",
       "american    1504\n",
       "us          1504\n",
       "take        1491\n",
       "life        1481\n",
       "psbattl     1470\n",
       "help        1442\n",
       "go          1418\n",
       "state       1409\n",
       "back        1369\n",
       "two         1364\n",
       "school      1345\n",
       "see         1329\n",
       "photo       1324\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in data_stemmed\n",
    "\n",
    "word_freq_stemmed = pd.Series(\" \".join(data_stemmed).split()).value_counts()\n",
    "word_freq_stemmed[1:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>darli</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cvtwtq</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cornerston</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>codina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sebasti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nearsight</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>poaq69oe7ti31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1741x2604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>110k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  freq\n",
       "0          darli     1\n",
       "1         cvtwtq     1\n",
       "2     cornerston     1\n",
       "3           soot     1\n",
       "4         codina     1\n",
       "5        sebasti     1\n",
       "6      nearsight     1\n",
       "7  poaq69oe7ti31     1\n",
       "8      1741x2604     1\n",
       "9           110k     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_stemmed[-10:].reset_index(name=\"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Balanced dataset'}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0 to 1 ratio\n",
    "data[\"label\"].value_counts(normalize=True).plot(kind='bar',title='Balanced dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data from test purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44826,)\n",
      "(14942,)\n"
     ]
    }
   ],
   "source": [
    "#split data_lemmatized feature\n",
    "\n",
    "# split the original training set to a train and a validation set becuase we will use them in search method\n",
    "\n",
    "X_train_1 , X_val_1 , Y_train_1, Y_val_1 = train_test_split(data_lemmatized,data['label'],stratify=data['label'], random_state=42, test_size=0.25, shuffle=True)\n",
    "\n",
    "\n",
    "split_index_lemmatized = [-1 if x in X_train_1.index else 0 for x in data_lemmatized.index]\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(X_val_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44826,)\n",
      "(14942,)\n"
     ]
    }
   ],
   "source": [
    "#split data_stemmed feature\n",
    "\n",
    "#Split the original training set to a train and a validation set becuase we will use them in search method\n",
    "\n",
    "X_train_2, X_val_2 ,Y_train_2, Y_val_2 = train_test_split(data_stemmed,data['label'],stratify=data['label'], random_state=42, test_size=0.25, shuffle=True)\n",
    "\n",
    "\n",
    "split_index_stemmed = [-1 if x in X_train_2.index else 0 for x in data_stemmed.index]\n",
    "\n",
    "\n",
    "print(X_train_2.shape)\n",
    "print(X_val_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start building our models and test each of them on each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial-1: Define a tunable pipeline with *TfidfVectorizer* and *logistic regression* model on stemmed data.\n",
    "\n",
    "we will train a pipeline on the whole dataset with pre-defined validation set.\n",
    "\n",
    "First things first: I convert each text the input column to numerical values using TfidfVectorizer then I tried to train them using logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "ps_1 = PredefinedSplit(split_index_stemmed)\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__analyzer\": ['word','char','char_wb'],\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True,False]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=ps_1, scoring=\"roc_auc\", n_iter=50)\n",
    "pipe_lg_clf.fit(data_stemmed, data['label'])\n",
    "\n",
    "pickle.dump(pipe_lg_clf, open(\"./clf_pipe.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8842549027632801\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 9, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best score {}'.format(pipe_lg_clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the best hyperparameter combination for <code>TfidfVectorizer()</code> and use them with the model to search for best hyperparameters combination for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this best hyperparameters for <code>TfidfVectorizer()</code>, we can search for optimal hyperparameters for the `logistic regression` classifier becuase thet will improve the classification results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "# define parameter space to test # runtime 2 min\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True], #True best\n",
    "    'tfidf__strip_accents':[None],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5)], #(1,2)\n",
    "    'tfidf__analyzer':['char'],\n",
    "    'tfidf__min_df': [11], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga'],\n",
    "    'lg__C': [1.0,0.1,0.001,0.0001,0.005,1.5,2.0,3.5],\n",
    "    'lg__fit_intercept':[False, True],\n",
    "\n",
    "}\n",
    "#    \n",
    "#   \n",
    "# it is quite slow so we do 4 for now\n",
    "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=ps_1, scoring=\"roc_auc\", n_iter=50)\n",
    "pipe_lg_clf.fit(data_stemmed, data['label'])\n",
    "\n",
    "pickle.dump(pipe_lg_clf, open(\"./clf_pipe.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8903333162307389\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 11, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char', 'lg__solver': 'newton-cg', 'lg__fit_intercept': True, 'lg__class_weight': 'balanced', 'lg__C': 3.5}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best score {}'.format(pipe_lg_clf.best_params_))\n",
    "\n",
    "# best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 12, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char', 'lg__solver': 'sag', 'lg__fit_intercept': False, 'lg__class_weight': None, 'lg__C': 3.5} 0.881381704490526\n",
    "# best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': True, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 5, 'tfidf__max_df': 0.3, 'tfidf__analyzer': 'word', 'lg__solver': 'sag', 'lg__fit_intercept': False, 'lg__C': 2.0} 0.8822453312417121\n",
    "# best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': True, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 5, 'tfidf__max_df': 0.3, 'tfidf__analyzer': 'word', 'lg__solver': 'lbfgs', 'lg__fit_intercept': False, 'lg__C': 2.0} 0.8823252722847258\n",
    "# best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': True, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__analyzer': 'word', 'lg__solver': 'sag', 'lg__fit_intercept': False, 'lg__C': 3.5} 0.8827944487105306\n",
    "# best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': True, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__analyzer': 'word', 'lg__penalty': 'l2', 'lg__fit_intercept': False, 'lg__C': 3.5} 0.8827951346447609\n",
    "# best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__analyzer': 'word', 'lg__solver': 'liblinear', 'lg__fit_intercept': False, 'lg__class_weight': 'balanced', 'lg__C': 3.5} 0.8834023255296484\n",
    "# best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 2), 'tfidf__analyzer': 'word', 'lg__solver': 'liblinear', 'lg__fit_intercept': False, 'lg__class_weight': 'balanced', 'lg__C': 3.5} 0.8837091356960659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters combinations and roc_auc score for <code>TfidfVectorizer()</code> with logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 11, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char', 'lg__solver': 'newton-cg', 'lg__fit_intercept': True, 'lg__class_weight': 'balanced', 'lg__C': 3.5}\n"
     ]
    }
   ],
   "source": [
    "best_params_lg = pipe_lg_clf.best_params_\n",
    "print(best_params_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', max_df=0.2, min_df=11,\n",
       "                                 ngram_range=(1, 5), smooth_idf=False,\n",
       "                                 sublinear_tf=True)),\n",
       "                ('lg',\n",
       "                 LogisticRegression(C=3.5, class_weight='balanced',\n",
       "                                    max_iter=10000, n_jobs=-1, random_state=42,\n",
       "                                    solver='newton-cg'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lg.set_params(**best_params_lg).fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = id\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_lg.predict_proba(test['text'])[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough_lg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions about first trial result:\n",
    "\n",
    "Here logisitc regression was able to get the best score on kaggle (roc_auc = 0.87006), the model didn't overfit after hyperparameter tuning with random search (I can improve the score metric if I use grid search to search for best hyperparameter combination), and maybe it would get better score if I tried on lemmatized data not stemmed one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial-2: Define a tunable pipeline with *TfidfVectorizer* and *Xgboosting* model on lemmatized data.\n",
    "\n",
    "First things first: I convert each text the input column to numerical values using TfidfVectorizer then I tried to train them using Xgboosting classifier and evaluate the model results using predefined validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric='rmse',\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_const...\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True, False]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"xgb\", XGBClassifier(random_state=42,n_jobs=-1,eval_metric='rmse',use_label_encoder=False))])\n",
    "\n",
    "ps_2 = PredefinedSplit(split_index_lemmatized)\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__analyzer':['word','char','char_wb'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True,False]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=ps_2, scoring=\"roc_auc\", n_iter=45)\n",
    "pipe_xgb_clf.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters combinations and roc_auc score for <code>TfidfVectorizer()</code> with XG boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.873591516190417\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
    "print('best score {}'.format(pipe_xgb_clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this best hyperparameters for <code>TfidfVectorizer()</code>, we can search for optimal hyperparameters for the `XGB` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric='rmse',\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type=None,\n",
       "                                                            interaction_const...\n",
       "                   n_iter=45, n_jobs=-1,\n",
       "                   param_distributions={'tfidf__analyzer': ['word', 'char',\n",
       "                                                            'char_wb'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [11],\n",
       "                                        'tfidf__ngram_range': [(1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': [None],\n",
       "                                        'tfidf__sublinear_tf': [True],\n",
       "                                        'xgb__booster': ['gbtree', 'gblinear',\n",
       "                                                         'dart'],\n",
       "                                        'xgb__learning_rate': [1.0, 0.1, 0.01,\n",
       "                                                               0.0001, 1.5]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"xgb\", XGBClassifier(random_state=42,n_jobs=-1, eval_metric='rmse',use_label_encoder=False))])\n",
    "\n",
    "# define parameter space to test # runtime 17 min\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True], #True best\n",
    "    'tfidf__strip_accents':[None],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5)], #(1,2)\n",
    "    'tfidf__min_df': [11], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['word','char','char_wb'],\n",
    "    'xgb__booster':['gbtree','gblinear', 'dart'],\n",
    "    'xgb__learning_rate':[1.0, 0.1,0.01,0.0001, 1.5],\n",
    "\n",
    "}\n",
    "   \n",
    "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=ps_2, scoring=\"roc_auc\", n_iter=45)\n",
    "pipe_xgb_clf.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8805476225528399\n",
      "best score {'xgb__learning_rate': 0.01, 'xgb__booster': 'gblinear', 'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 11, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
    "print('best score {}'.format(pipe_xgb_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__learning_rate': 0.01, 'xgb__booster': 'gblinear', 'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 11, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "best_params_xgb = pipe_xgb_clf.best_params_\n",
    "print(best_params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.2, min_df=11, ngram_range=(1, 5),\n",
       "                                 smooth_idf=False, sublinear_tf=True)),\n",
       "                ('xgb',\n",
       "                 XGBClassifier(base_score=0.5, booster='gblinear',\n",
       "                               colsample_bylevel=None, colsample_bynode=None,\n",
       "                               colsample_bytree=None, enable_categorical=False,\n",
       "                               eval_metric='rmse', gamma=None, gpu_id=-1,\n",
       "                               importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=0.01,\n",
       "                               max_delta_step=None, max_depth=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=None,\n",
       "                               predictor=None, random_state=42, reg_alpha=0,\n",
       "                               reg_lambda=0, scale_pos_weight=1, subsample=None,\n",
       "                               tree_method=None, use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_xgb.set_params(**best_params_xgb).fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = id\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_lg.predict_proba(test['text'])[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions about second trial result:\n",
    "\n",
    "Here xgboosting classifier score wasn't like the first trail **(roc_auc score = 0.86437)** although I passed a better data to it (lemmatized data), maybe I should try using grid search to search for the best hyperparameters combinations but that would take a lot of time to search for these hyperparamters combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 3: **TfidfVectorizer()** (*character-level*) with random forest classifier on lemmatized data.\n",
    "\n",
    "Here I made tf_idf and random forest classifier hyperparameters are selected by me to see if that imporve the score, and I use random forest classifier because it's much more better than xgboosting classifier in terms of computation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_df=0.2, min_df=10, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute unique word vector with frequencies exclude very uncommon (<10 obsv.) and common (>=20%) words use pairs of two words (ngram)\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"char\", max_df=0.2, min_df=10, ngram_range=(1, 3), norm=\"l2\"\n",
    ")\n",
    "tf_idf_vectorizer.fit(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word (ngram) vector extract:\n",
      "\n",
      " zul    9457\n",
      "sw     7998\n",
      "pag    7047\n",
      "mri    6173\n",
      "211    1053\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vector representation of vocabulary\n",
    "word_vector = pd.Series(tf_idf_vectorizer.vocabulary_).sample(5, random_state=1)\n",
    "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59768, 9475)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "data_lemmatized_vec = tf_idf_vectorizer.transform(data_lemmatized)\n",
    "X_test_vec = tf_idf_vectorizer.transform(test_lemmatized)\n",
    "data_lemmatized_vec.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=7, max_leaf_nodes=5, n_estimators=150)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=150,max_depth=7,max_leaf_nodes=5)\n",
    "model_rf.fit(data_lemmatized_vec,data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = id\n",
    "submission['label'] = model_rf.predict_proba(X_test_vec)[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions about third trial result:\n",
    "\n",
    "Random forest calssifier results was same as xgboosting results and I think because they are tree algorithms but random forest classifier is much more faster in training if I tried to fine tune the hyperparameters I think I would get higher results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 4: **CountVectorizer()** *(word-level)* with Naive bayes classifier *default hyperparameters* on stemmed data.\n",
    "\n",
    "Here I tried Naive bayes classifier with default hyperparameters but now I changed the vectorizer from tf_idf to countvectorizer to see which will do better for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute unique word vector with frequencies exclude very uncommon (<10 obsv.) and common (>=30%) words use pairs of two words (ngram)\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 3)\n",
    ")\n",
    "count_vectorizer.fit(data_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word (ngram) vector extract:\n",
      "\n",
      " peanut           7093\n",
      "duti             3155\n",
      "1492               73\n",
      "firearm          3724\n",
      "rudi giuliani    8227\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vector representation of vocabulary\n",
    "word_vector = pd.Series(count_vectorizer.vocabulary_).sample(5, random_state=1)\n",
    "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59768, 10741)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with count value as elements\n",
    "\n",
    "data_stemmed_vec = count_vectorizer.transform(data_stemmed)\n",
    "X_test_vec_2 = count_vectorizer.transform(test_stemmed)\n",
    "data_stemmed_vec.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training multinomial naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NV = MultinomialNB()\n",
    "model_NV.fit(data_stemmed_vec,data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = id\n",
    "submission['label'] = model_NV.predict_proba(X_test_vec_2)[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough_nv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions about fifth trial result:\n",
    "\n",
    "This trial's score was the worest **(roc_auc = 0.71165)**, and to check if this was vectorizer fault or classifier fault I tried another trial but not in this file with tf_id vectorizer and the same classifier and I noticed that the result increased and was close to the other results, so in kind of this problem count_vectorizer is not a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last trail: **TfidfVectorizer()** (*word-level*)  with XG boosting classifier on lemmatized data.\n",
    "\n",
    "Based on my friends advice, they said the XG boosting classifier result was good for them, so I tried to change the analyzer of tf_idf vectorizer here from 'character' to 'word' without any hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.4, min_df=10, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer_xgb = TfidfVectorizer(\n",
    "    analyzer=\"word\", max_df=0.4, min_df=10, ngram_range=(1, 2)\n",
    ")\n",
    "tf_idf_vectorizer_xgb.fit(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word (ngram) vector extract:\n",
      "\n",
      " operation barbarossa     7373\n",
      "good time                4653\n",
      "franchise                4333\n",
      "wwii propaganda         11512\n",
      "unarmed                 10841\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vector representation of vocabulary\n",
    "word_vector = pd.Series(tf_idf_vectorizer_xgb.vocabulary_).sample(5, random_state=1)\n",
    "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59768, 11608)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "\n",
    "data_stemmed_vec_xgb = tf_idf_vectorizer_xgb.transform(data_stemmed)\n",
    "X_test_vec_xgb = tf_idf_vectorizer_xgb.transform(test_stemmed)\n",
    "data_stemmed_vec_xgb.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training xgb classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='rmse', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=5, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=200, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(eval_metric='rmse',max_depth=5,n_estimators=200,use_label_encoder=False)\n",
    "model_xgb.fit(data_stemmed_vec_xgb,data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = id\n",
    "submission['label'] = model_xgb.predict_proba(X_test_vec_xgb)[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough_xgb2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions about last trial result:\n",
    "\n",
    "xgboosting calssifier results was better than second trial although I changed one parameter in the vectorizer which was analyzer **(roc_auc = 0.86350)**, by that I noticed this problem the best vectorizer to vectorize the data was tf_id vectorizer and the best analyzer was word, count_vectorizer in this problem wasn't good at all."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
